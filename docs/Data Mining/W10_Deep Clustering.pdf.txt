COMP5121
Data Mining and Data Warehousing Applications

Week 10: Clustering with Deep Learning Models

Dr. Fengmei Jin
ï‚§ Email: fengmei.jin@polyu.edu.hk
ï‚§ Office: PQ747 (+852 3400 3327)
ï‚§ Consultation Hours: 2.30-4.30pm every Thursday

Outline

ï¯ Clustering vs Deep Clustering
ï¯ Unsupervised, Autoencoder-based Clustering
ï¯ Self-supervised, Contrastive-based Clustering

2

Clustering

ï¯ Goal: group objects into clusters s.t.

ï® data objects within the same clusters are 
close to each other, while ensuring that 
different clusters are well-separated

ï¯ Most clustering algorithms rely on 

distance functions. 
ï® Distance measures (e.g., Euclidean) are 
meaningless in high-dimensional space 
ï¯ distance to nearest neighbors 
vs distance to farthest ones

3

Why Deep Clustering?

ï¯ High-Dimensional Data

ï® Classical clustering (e.g., k-Means) struggles when data is very high-

dimensional (images, text, etc.). 

ï® DNNs learn more compact, meaningful representations. 

ï¯ Nonlinear Feature Extraction

ï® Neural networks capture complex structures/patterns that are hard for 

traditional clustering algorithms to uncover. 

ï¯ Integrated Learning, Training, and Optimization

ï® The ability to train feature extraction and clustering objectives jointly 
often leads to better performance than a two-step â€œfeature extraction 
â†’ clusteringâ€ pipeline.

4

Deep Clustering

ï¯ Deep neural networks extract important features and capture 
the underlying structure of data ïƒ encoded representations 
that make clustering easier and more meaningful

ï¯ Typical Workflow

1. Representation learning (embedding): map high-dimensional 

data into a lower-dimensional latent space

2. Clustering in latent space: apply clustering algorithms (e.g., k-

means) to the learned embedding

3. *Iterative refinement: some methods alternate between feature 

learning and cluster assignment to iteratively improve both

5

(I) Unsupervised, Autoencoder-based Clustering

ï¯ Core Idea: Use an autoencoder to compress data into a latent 

space, then cluster those latent embeddings.

ï¯ Advantages: Keeps a reconstruction objective that preserves 

important data characteristics.

ï¯ Representative Methods:

[1] DEC (Deep Embedded Clustering)
[2] VaDE (Variational Autoencoder for Deep Embedding)
[3] IDEC (Improved Deep Embedded Clustering)

[1] Xie, Junyuan, et al. "Unsupervised deep embedding for clustering analysis." ICML, 2016.
[2] Jiang, Zhuxi, et al. "Variational deep embedding: An unsupervised and generative approach to clustering." IJCAI, 2017.
[3] Guo, Xifeng, et al. "Improved deep embedded clustering with local structure preservation." IJCAI, 2017.

6

(II) Self-supervised, Contrastive-based Clustering

ï¯ Core Idea: Learn latent representations by pushing apart 

dissimilar data samples and pulling together similar ones â€”
often via data augmentations.

ï¯ Advantages: Does not require labeled data; learned features 

tend to be robust and highlight semantic structure.

ï¯ Representative Methods:

[1] DeepCluster: Iterative clustering + CNN training
[2] SwAV: Online â€œclustersâ€ (prototypes) plus contrastive ideas
[3] SCAN: Contrastive learning via nearest-neighbor clustering

[1] Caron, Mathilde, et al. "Deep clustering for unsupervised learning of visual features." ECCV, 2018.
[2] Caron, Mathilde, et al. "Unsupervised learning of visual features by contrasting cluster assignments.â€œ NEURIPS, 2020.
[3] Van Gansbeke, Wouter, et al. "Scan: Learning to classify images without labels." ECCV, 2020.

7

UNSUPERVISED, AUTOENCODER-BASED CLUSTERING

8

Autoencoder for Representation Learning

ï¯ A neural network used to learn 
compact (lower-dimensional), 
informative embedding in an 
unsupervised way 
ï® encoder-decoder architecture

ï¯ Train the network to capture the 
most important patterns and 
reconstruct input data

The prediction (output) is a 
reconstruction of the input data.

9

Autoencoder: Architecture

ï¯ Encoder: A module that â€œcompressâ€ input data into a latent 

space with lower dimensions

ï¯ Decoder: A module that â€œdecompressâ€ the embeddings and 

â€œreconstructâ€ the data back

Model Training: Minimize 
the reconstruction loss

Latent 
Space

â€¢ Input: 
the 

data points in 
-dimensional space

â€¢ Output: cluster partitions

ğ‘ğ‘

ğ·ğ·

10

Autoencoder for Clustering

Joint optimization
â€¢ NN parameters 
â€¢ cluster parameters

11

Autoencoder: Toy Example

Clustering on 
unseen data

Training

12

Autoencoder-based Clustering

ï¯ Learning Objectives: minimize reconstruction error while 

optimizing cluster assignments.

ï¯ Strengths:

ï® Preserves global structure and data semantics
ï® Stable training dynamics and efficient for smaller datasets
ï® Performs well on structured data (e.g., tabular, time series)

ï¯ Limitations:

ï® Struggles with irrelevant details
ï® Requires careful architecture design

13

SELF-SUPERVISED, CONTRASTIVE-BASED CLUSTERING

14

Overview of Learning Paradigms

ï¯ Supervised Learning: Requires labeled data to learn mapping
ï® Example: image classification (Image â†’ Class label), sentiment 

analysis (Text â†’ Sentiment label)

ï¯ Unsupervised Learning: No labels are provided, focusing on 
discovering patterns or structures in the data â€“ Clustering
ï¯ Semi-Supervised Learning: Mix of labeled & unlabeled data
ï® To use a small set of labels and a large pool of unlabeled data
ï¯ Self-Supervised Learning: Generates its own â€œlabelsâ€ directly 

from the data itself, with no need for external annotation
ï® Example: predicting masked parts of an image or sentence

15

Contrastive Learning

ï¯ A self-supervised learning method that learns representations 
by contrasting similar (positive) and dissimilar (negative) data
ï® Encourage similar data points to have closer representations
ï® Pushes apart dissimilar data points in the feature space

ï¯ Representative methods:

[1] SimCLR: Maximizes agreement between augmented views of 
the same image.
[2] MoCo: Builds a memory bank for negative samples to improve 
contrastive learning, especially for large-scale data

[1] Chen, Ting, et al. "A simple framework for contrastive learning of visual representations." ICML, 2020.
[2] He, Kaiming, et al. "Momentum contrast for unsupervised visual representation learning." CVPR, 2020.

16

Data Augmentation in SimCLR

[1] Chen, Ting, et al. "A simple framework for contrastive learning of visual representations." ICML, 2020.

17

Contrastive Learning

Positive: diverse 
views of the same 
data points

Negative: views 
of different ones

https://www.v7labs.com/blog/contrastive-learning-guide

18

From Contrastive Learning to Clustering

ï¯ Contrastive-Based Clustering

ï® Combines the power of CL with clustering techniques
ï® Learns robust representations that align well with clustering 

objectives: similar data points are close & well-separated clusters

ï® Does not require explicit labels

ï¯ Typical Workflow

ï® Pretrain a model using a CL framework (e.g., SimCLR)
ï® Use the learned representations for clustering (e.g., K-means)
ï® Optionally refine the clusters iteratively using joint objectives

19

Contrastive Clustering

Wu, Yang, et al. "Unsupervised Clustering with Contrastive Learning for Rumor Tracking on Social Media." CCF International 
Conference on Natural Language Processing and Chinese Computing. Cham: Springer Nature Switzerland, 2023.

20

Summary

ï¯ Deep Clustering: handles high-dimensional data through 

representation learning (without relying on distance measures)

ï¯ Autoencoder-based Clustering: learns low-dimensional 

embeddings by encoding and reconstructing data
ï® Preserves global structure and is effective for structured data.
ï® Jointly minimizes reconstruction and clustering objectives.

ï¯ Contrastive-based Clustering: learns robust representations 
by contrasting similar (positive) and dissimilar (negative) pairs.
ï® Forms a feature space ideal for clustering.
ï® No need for explicit labels and generalizes well across tasks.

21

Email: fengmei.jin@polyu.edu.hk

Office: PQ747

THANK YOU!

22

