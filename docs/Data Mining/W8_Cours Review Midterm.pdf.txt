COMP5121
Data Mining and Data Warehousing Applications

Week 8: Course Review for Mid-term Exam

Dr. Fengmei Jin
ï‚§ Email: fengmei.jin@polyu.edu.hk
ï‚§ Office: PQ747 (+852 3400 3327)
ï‚§ Consultation Hours: 2:30-4:30pm every Thursday

The KDD process
KNOWLEDGE DISCOVERY FROM DATA

2

Knowledge discovery in databases

ï¯ KDD Steps

Data 
Mining

1. Evaluation (based on 

interestingness)

2. Presentation (e.g., 

Patterns

visualization)

Selection and 
Transformation

Data Cleaning 
and Integration

Task-relevant
Data

Data 
Warehouse

Databases

Feedbacks

3

Data Objects

ï¯ Databases/Datasets are made up of data objects.
ï¯ A data object represents an entity.

ï±Sales DB: customers, store items, sales
ï±Medical DB: patients, treatments
ï±University DB: students, professors, courses

ï¯ Database rows ïƒ  data objects, described by attributes

ï® Also called as samples, examples, instances, data points, tuples

ï¯ Database columns ïƒ  attributes

ï® Also called as data field, characteristic, dimension, feature, variable

4

Classify Attribute Types

ï¯ To describe a qualitative feature of an object that does not 
provide actual size or quantity â€“ nominal, binary, ordinal
ï® Values are typically words representing categories.
ï® Integers are used to embed categories as codes.
ï¯ 0 for small drink size, 1 for medium, and 2 for large.

ï¯ To provide quantitative measurements of an object â€“ numeric

ï® Interval-scaled: No true zero.
ï® Radio-scaled: True zero, enabling meaningful ratios.

5

Basic Statistical Descriptions of Data (I)

ï¯ Motivation: To better understand the data, identify properties of 
the data, and highlight what values shall be treated as noise
ï® Central tendency: to measure the middle or center of the data
ï¯ Mean: The average of the data (sensitive to extremes/outliers)
ï¯ Median: The middle value when data is ordered (a more robust measure 

when data is skewed)

ï¯ Mode: The most frequently occurring value

example: a strong 
middle class and fewer 
low-income households, 
e.g., Sweden, Finland, 
Denmark.

example: a small group 
of extremely high-income 
earners and a large 
population of low- to 
middle-income workers, 
e.g., New York, HK

6

Symmetric vs. Skewed Data

ï¯ Compare the central tendency (i.e., median, mean and mode) 
of symmetric, positively-skewed and negatively-skewed data

*the long tail is on 
the positive side 
(higher values)

7

Basic Statistical Descriptions of Data (II)

ï¯ Motivation: To better understand the data, identify properties of 
the data, and highlight what values shall be treated as noise
ï® Data dispersion: how are the data spread out?
ï¯ Range: difference between max and min values
ï¯ Interquartile Range (IQR): Measures spread around the median
ï¯ Variance / Standard Deviation: Indicate deviation from the mean

8

Measures of Data Dispersion

ï¯

-Quantiles: 

Q1
data points where the data distribution is split 
equal-size consecutive sets, e.g., 2-quantile (i.e., median), 

into 
ğ’’ğ’’
4-quantiles (called quartile), 100-quantiles (called percentiles)

ğ‘ğ‘ âˆ’ 1

Q3

ğ‘ğ‘

Interquartile Range (IQR)
â€¢

to identify the spread of the central 
portion of a dataset

â€¢ calculated as the difference between: 

â€¢ Upper quartile, Q3
â€¢ Lower quartile, Q1
â€¢ IQR = Q3 â€“ Q1 = 63 â€“ 47 = 16

9

Graphic Displays: Boxplot

ï¯ Quartiles (i.e., 4-quantiles)

ï® Five-number summary: min, Q1, 

median (Q2), Q3, max

ï® Boxplot: data is represented by a box
ï¯ IQR:  the two ends of the box are at Q1 
and Q3, i.e., the height of the box is IQR
ï¯ Median: marked by a line within the box 
ï¯ Whiskers: two lines outside the box 

extended to min and max

IQR
50%

10

Graphic Displays: Boxplotâ€™s Application

ï¯ Outliers

ï® data points beyond a 
specified threshold
ï¯ Usually, outside values are 
higher than Q3 or 

lower than Q1
1.5 Ã— ğ¼ğ¼ğ¼ğ¼ğ¼ğ¼
ï® Plotted individually

ï¯ The whiskers shall stop at 
the most extreme low/high 
observations within 
of the quartile.

ï¯ Then, outliers show up.

1.5 Ã—

ğ¼ğ¼ğ¼ğ¼ğ¼ğ¼

11

Why Preprocess the Data? Data Quality!

ï¯ Data quality depends on the intended use of data.
ï¯ Multidimensional views of data quality:

ï® Accuracy: data must correctly reflect the real-world 

scenario without errors or noise.

ï® Completeness: all required data fields should be 

present and valid.

ï® Consistencyï¼š data should follow the same rules 

and format across all records. 

ï® Timeliness: data should be up-to-date.
ï® Believability: data should be credible and from 

trusted sources.

ï® Interpretability: data should be clear and 

understandable.

12

Major Tasks of Data Preprocessing

ï¯ Data Cleaning

messy

clean

ï® To fill in missing data, smooth noisy data, identify or remove 

outliers, and resolve inconsistencies

ï¯ Data Integration (e.g., Bill Gates, William Gates, B. Gates, â€¦)

ï® To merge multiple databases into a coherent data store

ï¯ Data Reduction (efficiency of mining process)

ï® To obtain a reduced representation of the data with similar results

ï¯ Data Transformation

ï® To normalize data for similarity-based 

mining (e.g., age vs salary)

13

data cube
DATA WAREHOUSE AND OLAP

14

Why a Separate Data Warehouse?

ï¯ High performance for both systems:

â€¢ DBMS â€“ tuned for OLTP: access methods, indexing, hashing, concurrency control, recovery

â€¢ Warehouse â€“ tuned for OLAP: complex OLAP queries, consolidation, multi-dimensional view

ï¯ Different data and functions:

ï® Data warehouses are structured for analysis, with standardized 
schemas and consolidated information from diverse sources.
ï® Data warehouses support complex analytics on historical data. 

Operational databases handle frequent transactions and updates. 
ï¯ Some systems perform OLAP directly on DBs, but performance 

and scalability may be limited.

15

From Tables and Spreadsheets to Data Cubes

ï¯ A data warehouse is based on a multi-dimensional data mode,  

which views data in the form of a data cube, defined by:
ï® Dimension tables: to describe a dimension, e.g., item (item_name, 

brand, type), or time (day, week, month, quarter, year) 

ï® Fact table: to store numeric measures (e.g., dollars_sold) and keys 

linking to dimension tables â€“ analyze relationships between dimensions 

ï¯ Data cube is typically 

-dimensional.

ï® The 
ï® The topmost 
ğ‘›ğ‘›

ğ‘›ğ‘›

-dimensional base cube is called a base cuboid.

-dimensional cuboid, which provides the highest-level 

summarization, is called the apex cuboid.
ï® All levels of cuboids form the entire data cube.

0

16

Example: Structure of Data Cube

17

Schemas for Multi-dimensional Data Models

ï¯ Entity-Relationship (ER) model and the schema

ï® a set of entities and their relationships â€“ appropriate for OLTP

Stu_Id

Stu_name

Cou_Id

Cou_name

Student

study

Course

ï¯ A multi-dimensional model for data warehouses: focus on 

dimensions and measures, in the form of:
ï® star schema, snowflake schema, fact constellation schema

18

(1) Star Schema

A fact table in the center, surrounded 
by a set of dimension tables 

time

time_key
day
day_of_the_week
month
quarter
year

branch

branch_key
branch_name
branch_type

Measures

Sales (Fact Table)

time_key

item_key

branch_key

location_key

units_sold

dollars_sold

avg_sales

item

item_key
item_name
brand
type
supplier_type

location

location_key
street
city
state_or_province
country

19

A Sample Data Cube

TV

PC

VCR

sum

Quarter

1Qtr

2Qtr

3Qtr

4Qtr

sum

Total annual sales 
of TVs in U.S.A.

U.S.A

Canada

Mexico

y
r
t
n
u
o
C

sum

Sales volume as a function of 
product, quarter, and country

Industry   Region         Year

Category   Country Quarter

Product

City     Month    Week

Office         Day

20

Typical OLAP Operations

ï¯ Roll up (drill-up): summarize data by climbing up hierarchy or by 

dimension reduction techniques

ï¯ Drill down (roll-down): reverse of roll-up

ï® from higher-level summary to lower-level summary or detailed data, or 

introducing new dimensions

ï¯ Slice and dice: project and select 
ï¯ Pivot (rotate): reorient the cube, visualization, 3D to series of 2D planes
ï¯ Other operations:

ï® Drill-across: involving (across) more than one fact table
ï® Drill-through: through the bottom level of the cube to its back-end 

relational tables (using SQL)

21

Typical OLAP Operations

ï¯ Roll up (drill-up)

ï® summarize data by 

climbing up hierarchy for 
a dimension or by 
dimension reduction

ï¯ Dice

ï® define a subcube by 

performing a selection
on two or more 
dimensions

Typical OLAP Operations

ï¯ Roll-down (drill-down): 

reverse of roll-up
ï® from higher-level 

summary to lower-level 
summary or detailed 
data, or introducing new 
dimensions

ï¯ Slice: define a subcube by 
performing a selection on 
one dimension

ï¯ Pivot (rotate): reorient the 
cube, visualization, 3D to 
series of 2D planes

A Star-Net Query Model

Shipping Method

Customer Orders

Customer

AIR-EXPRESS

CONTRACTS

TRUCK

ORDER

These represent the 
granularities available for 
use by OLAP operations. 

Time

YEAR

QUARTER

WEEK

NAME

BRAND

TYPE

Product

CITY

COUNTRY

REGION

Location

SALESPERSON

DISTRICT

DIVISION

Promotion

Organization

26

Efficient Data Cube Computation

ï¯ Data cube can be viewed as a lattice of cuboids  

ï® The bottom-most cuboid is the base cuboid â€“ the most specific
ï® The top-most cuboid (apex) contains only one cell â€“ the most 

generalized (all)

â€¢ Drilling down: start from apex cuboid and explore downward
â€¢ Rolling up: start at the base cuboid and explore upward

â€¢ 0-D op: i.e., no group-by SQL, like â€œcompute 

the sum of total salesâ€

â€¢ 1-D op: one group-by, e.g., â€œcompute the sum 

of sales, group-by cityâ€

â€¢ â€¦
â€¢ The cube operator is the 

-dimensional 

generalization of the group-by operator.

ğ‘›ğ‘›

27

Apriori algorithm, support, confidence, â€¦
FREQUENT ITEMSETS & ASSOCIATION 
RULE MINING

28

Basic Concepts: Frequent Itemsets

ï¯ Itemset: A set of one or more items

ï®

-itemset: 
ï¯ Support of an itemset

ğ’Œğ’Œ

ğ‘‹ğ‘‹ = ğ‘¥ğ‘¥1, â€¦ , ğ‘¥ğ‘¥ğ‘˜ğ‘˜

ğ‘˜ğ‘˜

with 

items

ï® Absolute Support (Count): the number of transactions 

containing the given itemset 

ï® Relative Support: the fraction of transactions containing 

(i.e., 

the probability that a transaction contains 

ğ‘‹ğ‘‹

)

ğ‘‹ğ‘‹

ï¯ Frequent Itemset: An itemset 

ğ‘‹ğ‘‹

is frequent if the support of 

is no less than 

â€“ a minsup threshold.

ğ‘‹ğ‘‹

ğœğœ

ğ‘‹ğ‘‹

29

The Apriori Algorithm: Framework

ï¯ Outline of Apriori: level-wise, candidate generation and test 

ïƒ˜ Initially, scan DB once to get frequent 1-itemset
ïƒ˜ Repeat

â€¢ Generate length-
â€¢ Test the candidates against DB to find frequent 
â€¢ Set 

ğ‘˜ğ‘˜ + 1

candidate itemsets based on frequent 

-itemsets

-itemsets
ğ‘˜ğ‘˜

ğ‘˜ğ‘˜ + 1

ïƒ˜ Until no frequent or candidate set can be generated
ïƒ˜ Return all the frequent itemsets derived

ğ‘˜ğ‘˜ â‰” ğ‘˜ğ‘˜ + 1

30

From Frequent Itemsets to Association Rules

ï¯ Association Rules written as X ïƒ  Y [support, confidence]

ï® Both 
are non-empty itemsets, and 
ï® It describes an â€˜if-thenâ€™ relationship between two sets of items.

and 

.

ğ‘‹ğ‘‹

ğ‘Œğ‘Œ

ğ‘‹ğ‘‹ âˆ© ğ‘Œğ‘Œ = âˆ…

ï® Support: The percentage of transactions containing both X and Y

ï¯

: the percentage of transactions that contains every item in 

, i.e., how frequently both 

appear together in the dataset
ğ‘‹ğ‘‹
ï® Confidence: The conditional probability that a transaction having 
ğ‘‹ğ‘‹

and 
ğ‘ƒğ‘ƒ(ğ‘‹ğ‘‹ âˆª ğ‘Œğ‘Œ)
ğ‘Œğ‘Œ

ğ‘Œğ‘Œ

sup(ğ‘‹ğ‘‹ â†’ ğ‘Œğ‘Œ) = P(ğ‘‹ğ‘‹ âˆª ğ‘Œğ‘Œ)
and 

also contains 

, that is,

ğ‘‹ğ‘‹

ğ‘Œğ‘Œ

conf ğ‘‹ğ‘‹ â†’ ğ‘Œğ‘Œ = P ğ‘Œğ‘Œ ğ‘‹ğ‘‹ = sup(ğ‘‹ğ‘‹ â†’ ğ‘Œğ‘Œ)/sup(ğ‘‹ğ‘‹)

31

Final Step: Rule Generation via Frequent Itemsets

ï¯ Support (min-sup): used to mine the frequent itemsets
ï¯ Confidence (min-conf): used by the rule generation step to 

qualify the strength of the derived association rules
ï® For each frequent itemset 
ï® For every non-empty subset 
ğ¹ğ¹

, generate a rule:
ğ¹ğ¹

, generate 

â€™s all non-empty subsets

satisfies the minimum confidence, i.e., 

ğ‘ ğ‘ 

ğ¼ğ¼: ğ‘ ğ‘  â†’ ğ¹ğ¹ âˆ’ ğ‘ ğ‘ 

is a strong association rule and should be output.

conf ğ‘ ğ‘  â†’ ğ¹ğ¹ âˆ’ ğ‘ ğ‘  =

sup ğ¹ğ¹
sup ğ‘ ğ‘  â‰¥ ğ‘šğ‘šğ‘šğ‘šğ‘›ğ‘› _ğ‘ğ‘ğ‘ğ‘ğ‘›ğ‘›ğ‘ğ‘

ï® If the rule 

ğ¼ğ¼

then 

ğ¼ğ¼

32

Limitation of the Support-Confidence Framework

ï¯ Strong rules are not necessarily interesting: â€œ
ï¯ Example: Suppose a school may have the following statistics 

â€ 

on # students related to playing basketball and/or eating cereal:

ğ´ğ´ â†’ ğµğµ

ğ‘ ğ‘ , ğ‘ğ‘

Play basketball

Not play basketball

Eat cereal

Not eat cereal

sum

400

200

600

350

50

400

sum

750

250

1000 (TOTAL)

ï® Association rule mining may generate a rule:

play-basketball ïƒ  eat-cereal [40%, 66.7%]

ï® But this strong association rule is misleading ïƒ  The overall % of 

students eating cereal is 75% > 66.7%. 

ï® A more telling rule: 

not play-basketball ïƒ  eat-cereal [35%, 87.5%] (high 

& 

)

33

ğ‘ ğ‘ 

ğ‘ğ‘

Interestingness Measure: Lift

ï¯ Measure of dependent / correlated events:

ï® Tell how 
ğ’ğ’ğ’ğ’ğ’ğ’ğ’ğ’ ğ‘©ğ‘©, ğ‘ªğ‘ª =

ï¯
ï¯
ï¯

ğ‘™ğ‘™ğ‘šğ‘šğ‘ğ‘ğ‘™ğ‘™ ğµğµ, ğ¶ğ¶ = 1
ğ‘™ğ‘™ğ‘šğ‘šğ‘ğ‘ğ‘™ğ‘™ ğµğµ, ğ¶ğ¶ > 1
ğ‘™ğ‘™ğ‘šğ‘šğ‘ğ‘ğ‘™ğ‘™ ğµğµ, ğ¶ğ¶ < 1

ï® Example:
=CB
)

,

lift

(

600
/
ï¯ Thus, 
ï¯
and 

P ğµğµ âˆª ğ¶ğ¶
and 
ğ‘ƒğ‘ƒ ğµğµ ğ‘ƒğ‘ƒ ğ¶ğ¶
: 
: positively correlated
: negatively correlated

sup(ğµğµ â†’ ğ¶ğ¶)
=
are correlated
sup ğµğµ sup(ğ¶ğ¶)
and 
ğ¶ğ¶
ğµğµ

=
are independent

ğµğµ

ğ¶ğ¶

conf ğµğµ â†’ ğ¶ğ¶
sup ğ¶ğ¶
C
Not C

400

200

sum

600

B

Not B

sum

350

50

400

750

250

1000

is more telling than s & c

400
1000

/
1000
Ã—
750
and 

=

89.0

lift

(

/

1000

=Â¬CB

)

,

600

/

ğ’ğ’ğ’ğ’ğ’ğ’ğ’ğ’

are negatively correlated since 

200
1000

1000
/
Ã—
250
.

=

33.1

/

1000

are positively correlated since 

ğ¶ğ¶

ğ‘™ğ‘™ğ‘šğ‘šğ‘ğ‘ğ‘™ğ‘™ ğµğµ, ğ¶ğ¶ < 1

.

34

ğ‘™ğ‘™ğ‘šğ‘šğ‘ğ‘ğ‘™ğ‘™ ğµğµ, Â¬ğ¶ğ¶ > 1

ğµğµ
Â¬ğ¶ğ¶

ğµğµ

Interestingness Measure: 

ï¯ To test correlated events: 

â€¢
â€¢

2

: independent
: correlated, either positive or 

=

ğœ’ğœ’

2

ğœ’ğœ’
negative ïƒ  needs additional test
ğœ’ğœ’

= 0
> 0

ğŸğŸ
ğŒğŒ
âˆ‘ ğ‘‚ğ‘‚ğ‘‚ğ‘‚ğ‘ ğ‘ ğ‘‚ğ‘‚ğ‘‚ğ‘‚ğ‘‚ğ‘‚ğ‘‚ğ‘‚ğ‘‚ğ‘‚ âˆ’ğ¸ğ¸ğ¸ğ¸ğ¸ğ¸ğ‘‚ğ‘‚ğ¸ğ¸ğ¸ğ¸ğ‘‚ğ‘‚ğ‘‚ğ‘‚
B
ğ¸ğ¸ğ¸ğ¸ğ¸ğ¸ğ‘‚ğ‘‚ğ¸ğ¸ğ¸ğ¸ğ‘‚ğ‘‚ğ‘‚ğ‘‚

2

2

Not B

C

400 (450) 350 (300)

Not C

200 (150)

50 (100)

sum

600

400

sum

750

250

1000

ï¯ Thus, 

and 

are negatively correlated since the expected

Expected value

Observed value

value is 450 but the observed is only 400.

ğµğµ

ğ¶ğ¶

is also more telling than the support-confidence framework

35

ï¯

2

ğœ’ğœ’

Lift and 

: Are They Always Good Measures?

ï¯ Null transactions: Transactions that contain neither 

2

ğœ’ğœ’

Examine the dataset:
â€¢

is much rarer than 

nor 

ğ‘©ğ‘©

ğ‘ªğ‘ª

and 
ğµğµğ¶ğ¶ 100, 0.1%
& 

â€¢ There are many 
â€¢ Unlikely 

Â¬ğµğµğ¶ğ¶ 1000

.
will happen together!
Â¬ğµğµÂ¬ğ¶ğ¶ 100000, 98%

ğµğµÂ¬ğ¶ğ¶ 1000

ğµğµ

ğ¶ğ¶

ï± However, B and C seem to be strongly positively 

correlated based on:

ï±
ï±

and Observed (100) >> 

2

ğ‘™ğ‘™ğ‘šğ‘šğ‘ğ‘ğ‘™ğ‘™ ğµğµ, ğ¶ğ¶ = 8.44 â‰« 1
Expected (11.85)
ğœ’ğœ’

(ğµğµ, ğ¶ğ¶) = 670

null transactions

Contingency table with 
expected values added

ï± Too many null transactions may â€œspoil the soupâ€!

36

Interestingness Measures: Null-Invariant

ï¯ Null invariance: value does not change with # null-transactions

ï®

and 

are NOT null-invariant with the range of 

.

ï¯ Null-invariant Measures:
ğ‘™ğ‘™ğ‘šğ‘šğ‘ğ‘ğ‘™ğ‘™

ğœ’ğœ’

2

0, âˆ

ï® All Confidence: the minimum confidence of the two association rules 

related to A and B, namely, â€œA ïƒ  Bâ€ and â€œB ïƒ  Aâ€

ï® Max Confidence: the maximum confidence of the two rules
ï® Kulczynski (Kulc): an average of two confidence values
ï® Cosine: a harmonized lift measure (unaffected by # total transactions)

37

Decision tree: ID3 algorithm driven by entropy and information gain
CLASSIFICATION

38

Decision Tree Structure

ï¯ A flow-chart-like structure used for classification

ï® Internal node: a test on an attribute (e.g., age, exercise, weight, 

smoking)

ï® Branch: an outcome of the test
ï® Leaf nodes: class labels (e.g., high-, moderate-, and low-risk)

How it works:
An object is classified 
by traversing the tree
from its root to a leaf.

Age 

=<20 

>20 & =< 50 

>50 

Low 

Obese 

Weight 

Smoking 

Over 

Normal 

Under 

No 

Yes 

Exercise 

Moderate 

Low 

Low 

Moderate 

High 

Never 

Regular 

Seldo m 

High 

High  Moderate 

39

 
Entropy

ï¯ A measure of randomness, uncertainty, and disorder in a 

system with probability distributions of outcome. 

ï¯ Entropy is formulated as a function that measures disorder.

ï® â€œThe higher the entropy, the greater the disorder.â€
ï® For classification, it tells how diverse the classes are in a set.

ï¯ Let 

be a set of examples from 

classes.

ğ‘«ğ‘«

ğ‘šğ‘š

ğ’ğ’

â€¢ Input: Distribution of outcomes
â€¢ Output : A value indicating how disordered the outcomes are
â€¢

: The proportion of examples observed in 

ğ¼ğ¼ğ‘›ğ‘›ğ‘ğ‘ğ‘ğ‘ ğ·ğ· = âˆ’ ï¿½
ğ‘–ğ‘–=1

ğ‘ğ‘ğ‘–ğ‘– ï¿½ log2 ğ‘ğ‘ğ‘–ğ‘–

that belong to i-th class within [0,1].

ğ‘ğ‘ğ‘–ğ‘–

ğ·ğ·

40

Example: Tossing Coins in Casino

ï¯ Casino A with real coins (50/50 chances):

ğ¼ğ¼ğ‘›ğ‘›ğ‘ğ‘ğ‘ğ‘ ğ¶ğ¶ğ‘ğ‘ğ‘šğ‘šğ‘›ğ‘› ğ‘‡ğ‘‡ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  = âˆ’ğ‘ğ‘ â„ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ log2 ğ‘ğ‘ â„ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ âˆ’ ğ‘ğ‘ ğ‘™ğ‘™ğ‘’ğ‘’ğ‘šğ‘šğ‘™ğ‘™ log2 ğ‘ğ‘ ğ‘™ğ‘™ğ‘’ğ‘’ğ‘šğ‘šğ‘™ğ‘™

= âˆ’

1
2

log2 1
2

âˆ’

1
2

log2 1
2

= 1

ï¯ Casino B with fake coins (75/25 chances):

ğ¼ğ¼ğ‘›ğ‘›ğ‘ğ‘ğ‘ğ‘ ğ¶ğ¶ğ‘ğ‘ğ‘šğ‘šğ‘›ğ‘› ğ‘‡ğ‘‡ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  = âˆ’ğ‘ğ‘ â„ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ log2 ğ‘ğ‘ â„ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ âˆ’ ğ‘ğ‘ ğ‘™ğ‘™ğ‘’ğ‘’ğ‘šğ‘šğ‘™ğ‘™ log2 ğ‘ğ‘ ğ‘™ğ‘™ğ‘’ğ‘’ğ‘šğ‘šğ‘™ğ‘™
1
4
Entropy is a measure of randomness and disorder. 
Higher entropy means higher uncertainty.

log2 1
4

log2 3
4

= 0.811

= âˆ’

3
4

âˆ’

41

Information Gain and Iterative Dichotomiser (ID3) 

ï¯ Classification Goal: To split the dataset in a way that reduces 

entropy the most.

ï¯ Information Gain: To measure the reduction in entropy after 

splitting the dataset on an attribute 

ï® Weighted entropy after split: 

ï¯

: subsets of 

ğ·ğ·ğ‘—ğ‘—

ğºğºğ‘’ğ‘’ğ‘šğ‘šğ‘›ğ‘› ğ·ğ·, ğ´ğ´ = ğ¼ğ¼ğ‘›ğ‘›ğ‘ğ‘ğ‘ğ‘ ğ·ğ· â€“ ğ¼ğ¼ğ‘›ğ‘›ğ‘ğ‘ğ‘ğ‘ğ´ğ´ ğ·ğ·
ğ¼ğ¼ğ‘›ğ‘›ğ‘ğ‘ğ‘ğ‘ğ´ğ´ ğ·ğ· = âˆ‘ğ‘—ğ‘—=1
ğ´ğ´

created by splitting on 

ğ·ğ·

ğ‘›ğ‘›

ğ´ğ´

ğ‘ğ‘ ğ·ğ·ğ‘—ğ‘— ğ´ğ´ ğ¼ğ¼ğ‘›ğ‘›ğ‘ğ‘ğ‘ğ‘ ğ·ğ·ğ‘—ğ‘—

ID3 Algorithm: Repeatedly selects the attribute with the highest 
information gain at each step to build the decision tree.

42

ID3 Example (Decision: buy computer or not)

ï¯ Class P: buys_computer = â€˜yesâ€™ ïƒ  9
ï¯ Class N: buys_computer = â€˜noâ€™ ïƒ  5

â€¢
â€¢
â€¢

ğˆğˆğˆğˆğˆğˆğˆğˆ ğ·ğ· = âˆ‘ âˆ’ğ‘ğ‘ğ‘–ğ‘– Ã— log2ğ‘ğ‘ğ‘–ğ‘–
ğˆğˆğˆğˆğˆğˆğˆğˆğ‘¨ğ‘¨ ğ·ğ· = âˆ‘ ğ‘ğ‘ ğ·ğ·ğ‘—ğ‘— ğ´ğ´ Ã— ğˆğˆğˆğˆğˆğˆğˆğˆ ğ·ğ·ğ‘—ğ‘—
ğ†ğ†ğ†ğ†ğ†ğ†ğˆğˆ ğ·ğ·, ğ´ğ´ = ğˆğˆğˆğˆğˆğˆğˆğˆ ğ·ğ· â€“ ğˆğˆğˆğˆğˆğˆğˆğˆğ‘¨ğ‘¨ ğ·ğ·

income student
age
high
<=30
<=30
high
31â€¦40 high
medium
>40
low
>40
>40
low
31â€¦40 low
<=30
<=30
>40
<=30
31â€¦40 medium
31â€¦40 high
>40

no
no
no
no
yes
yes
yes
no
medium
low
yes
medium yes
medium yes
no
yes
no

medium

credit_rating

fair
excellent
fair
fair
fair
excellent
excellent
fair
fair
fair
excellent
excellent
fair
excellent

buys_computer
no
no
yes
yes
yes
no
yes
no
yes
yes
yes
yes
yes
no

Info

(

= ID
)

)5,9(

âˆ’=

9
14

log

(

2

9
14

)

âˆ’

5
14

log

(

2

5
14

940.0)

=

age
<=30
31â€¦40
>40

pi
2
4
3
5
14

I(pi, ni)

ni
3 0.971
0 0
2 0.971

Infoage

(

D

)

=

5
14

I

)3,2(

+

4
14

I

)0,4(

+

5
14

I

.0)2,3(

=

694

I

)3,2(

means â€˜age <=30â€™ has 5 out of 14 

samples, with 2 â€˜yesâ€™ and 3 â€˜noâ€™. 

Hence,

Gain

(

age

)

=

Info

(

D

)

âˆ’

Info

age

(

D

246.0)

=

Similarly,

ğºğºğ‘’ğ‘’ğ‘šğ‘šğ‘›ğ‘›(ğ‘šğ‘šğ‘›ğ‘›ğ‘ğ‘ğ‘ğ‘ğ‘šğ‘šğ‘’ğ‘’) = 0.029
ğºğºğ‘’ğ‘’ğ‘šğ‘šğ‘›ğ‘›(ğ‘ ğ‘ ğ‘™ğ‘™ğ‘ ğ‘ ğ‘’ğ‘’ğ‘’ğ‘’ğ‘›ğ‘›ğ‘™ğ‘™) = 0.151

ğºğºğ‘’ğ‘’ğ‘šğ‘šğ‘›ğ‘›(ğ‘ğ‘ğ‘ğ‘ğ‘’ğ‘’ğ‘’ğ‘’ğ‘šğ‘šğ‘™ğ‘™_ğ‘ğ‘ğ‘’ğ‘’ğ‘™ğ‘™ğ‘šğ‘šğ‘›ğ‘›ğ‘Ÿğ‘Ÿ) = 0.048

Bayesian Theorem

ï¯

: Posterior probability, the probability of 

holds given 

: Evidences (e.g., a data tuple) with attribute description
: Hypothesis to be verified (e.g., a class label that 

ğ»ğ»

belongs to)

ğ¸ğ¸

ï®
ğ‘ƒğ‘ƒ ğ»ğ» ğ¸ğ¸
ï®
ğ¸ğ¸
ğ»ğ»

ğ¸ğ¸

ï¯

ï¯

ï¯

: prior probability, i.e., the initial probability of hypothesis 

ğ‘·ğ‘· ğ‘¬ğ‘¬|ğ‘¯ğ‘¯ ğ‘·ğ‘· ğ‘¯ğ‘¯
ğ‘·ğ‘· ğ‘¬ğ‘¬
: marginal probability, i.e., the total probability of observing evidence 

ğ‘·ğ‘· ğ‘¯ğ‘¯ ğ‘¬ğ‘¬ =

observing evidence 
ğ‘ƒğ‘ƒ ğ»ğ»

before 

ğ»ğ»

under all possible hypotheses
ğ‘ƒğ‘ƒ ğ¸ğ¸

ğ¸ğ¸

: likelihood, i.e., the probability of observing evidence 

given that the 

ğ¸ğ¸

hypothesis 
ğ‘ƒğ‘ƒ ğ¸ğ¸|ğ»ğ»

ğ»ğ» = ğ‘™ğ‘™ğ‘ğ‘ğ‘ ğ‘ ğ‘’ğ‘’

ğ¸ğ¸

44

Bayesian Classification

ï¯ A data tuple: 
ï¯ To classify 

, we need to estimate 
ğ‘‹ğ‘‹ = (ğ´ğ´1 = ğ‘¥ğ‘¥1, ğ´ğ´2 = ğ‘¥ğ‘¥2, ğ´ğ´3 = ğ‘¥ğ‘¥3, â€¦ , ğ´ğ´ğ‘›ğ‘› = ğ‘¥ğ‘¥ğ‘›ğ‘›)

represents the hypothesis that 

belongs to 

.

ï®
ï® We say 

ğ¶ğ¶ğ‘–ğ‘–

ğ‘‹ğ‘‹
belongs to 

iff: 

ğ‘ƒğ‘ƒ ğ¶ğ¶ğ‘–ğ‘– ğ‘‹ğ‘‹)
ğ‘‹ğ‘‹

ğ¶ğ¶ğ‘–ğ‘–

ï¯ How to estimate 

ğ‘‹ğ‘‹

ğ¶ğ¶ğ‘–ğ‘–

ğ‘ƒğ‘ƒ ğ¶ğ¶ğ‘–ğ‘–|ğ‘‹ğ‘‹ > ğ‘ƒğ‘ƒ ğ¶ğ¶ğ‘—ğ‘—|ğ‘‹ğ‘‹ , ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ ğ‘’ğ‘’ğ‘™ğ‘™ğ‘™ğ‘™ ğ‘—ğ‘— â‰  ğ‘šğ‘š
?

for classifying 

ï® Bayesian theorem: 

ğ‘ƒğ‘ƒ ğ¶ğ¶ğ‘–ğ‘– ğ‘‹ğ‘‹)

ğ‘ƒğ‘ƒ ğ‘‹ğ‘‹|ğ¶ğ¶ğ‘–ğ‘– ğ‘ƒğ‘ƒ ğ¶ğ¶ğ‘–ğ‘–
ï® The problem becomes ïƒ  estimating 
ğ‘ƒğ‘ƒ ğ‘‹ğ‘‹

ğ‘ƒğ‘ƒ ğ¶ğ¶ğ‘–ğ‘– ğ‘‹ğ‘‹ =

ğ‘‹ğ‘‹

and 

ğ‘ƒğ‘ƒ ğ‘‹ğ‘‹|ğ¶ğ¶ğ‘–ğ‘–

ğ‘ƒğ‘ƒ ğ¶ğ¶ğ‘–ğ‘–

45

Bayesian Classification

ï¯ Estimate the priori probability of the i-th class 

from the 

training set 

: 

ğ·ğ·

ğ‘ƒğ‘ƒ ğ¶ğ¶ğ‘–ğ‘– =

ğ¶ğ¶ğ‘–ğ‘–
ğ·ğ·

ğ¶ğ¶ğ‘–ğ‘–

ï¯ Independence Assumption: For 

, we assume that the 

effect of each attribute 

ğ´ğ´ğ‘—ğ‘—

is independent to others:

ğ‘ƒğ‘ƒ ğ‘‹ğ‘‹ | ğ¶ğ¶ğ‘–ğ‘–

where 

ğ‘ƒğ‘ƒ ğ‘‹ğ‘‹ = (ğ´ğ´1 = ğ‘¥ğ‘¥1, ğ´ğ´2 = ğ‘¥ğ‘¥2, â€¦ , ğ´ğ´ğ‘›ğ‘› = ğ‘¥ğ‘¥ğ‘›ğ‘›) ğ¶ğ¶ğ‘–ğ‘–)
= ğ‘ƒğ‘ƒ ğ´ğ´1 = ğ‘¥ğ‘¥1 ğ¶ğ¶ğ‘–ğ‘– Ã— ğ‘ƒğ‘ƒ ğ´ğ´2 = ğ‘¥ğ‘¥2 ğ¶ğ¶ğ‘–ğ‘– Ã— â‹¯
Ã— ğ‘ƒğ‘ƒ ğ´ğ´ğ‘›ğ‘› = ğ‘¥ğ‘¥ğ‘›ğ‘› ğ¶ğ¶ğ‘–ğ‘–

can also be estimated from the training set 

.

ğ‘ƒğ‘ƒ ğ´ğ´ğ‘—ğ‘— = ğ‘¥ğ‘¥ğ‘—ğ‘— ğ¶ğ¶ğ‘–ğ‘–)

ğ·ğ·

46

Example

Bayesian: 

ğ‘ƒğ‘ƒ ğ¶ğ¶ğ‘–ğ‘– ğ‘‹ğ‘‹ =

ğ‘ƒğ‘ƒ ğ‘‹ğ‘‹|ğ¶ğ¶ğ‘–ğ‘– ğ‘ƒğ‘ƒ ğ¶ğ¶ğ‘–ğ‘–
ğ‘ƒğ‘ƒ ğ‘‹ğ‘‹

ï¯ Given a training set, predict if a person

will buy a computer

: {age = youth, income = medium, student = yes, credit_rating = fair}

ğ‘‹ğ‘‹

ï®
ï® Yes or No? 

ğ‘¿ğ‘¿

Priori Probability in training Data:
â€¢
â€¢

ğ‘ƒğ‘ƒ ğ‘ğ‘ğ‘ ğ‘ ğ‘ğ‘_ğ‘ğ‘ğ‘ğ‘ğ‘šğ‘šğ‘ğ‘ğ‘ ğ‘ ğ‘™ğ‘™ğ‘’ğ‘’ğ‘ğ‘ ğ‘‹ğ‘‹

ğ‘ƒğ‘ƒ ğ‘ğ‘ğ‘ ğ‘ ğ‘ğ‘_ğ‘ğ‘ğ‘ğ‘ğ‘šğ‘šğ‘ğ‘ğ‘ ğ‘ ğ‘™ğ‘™ğ‘’ğ‘’ğ‘ğ‘ = ğ‘ğ‘ğ‘’ğ‘’ğ‘ ğ‘  = 9/14 = 0.643
ğ‘ƒğ‘ƒ ğ‘ğ‘ğ‘ ğ‘ ğ‘ğ‘_ğ‘ğ‘ğ‘ğ‘ğ‘šğ‘šğ‘ğ‘ğ‘ ğ‘ ğ‘™ğ‘™ğ‘’ğ‘’ğ‘ğ‘ = ğ‘›ğ‘›ğ‘ğ‘ = 5/14 = 0.357

:

To calculate 
â€¢
â€¢
â€¢
â€¢
ïƒ 
ïƒ  Similarly, 

ğ‘·ğ‘· ğ‘¿ğ‘¿ ğ’ƒğ’ƒğ’ƒğ’ƒğ’ƒğ’ƒ_ğ’„ğ’„ğ’„ğ’„ğ’ğ’ğ’„ğ’„ğ’ƒğ’ƒğ’ğ’ğ’„ğ’„ğ’„ğ’„ = ğ’ƒğ’ƒğ’„ğ’„ğ’šğ’š)

ğ‘ƒğ‘ƒ ğ‘’ğ‘’ğ‘Ÿğ‘Ÿğ‘’ğ‘’ = ğ‘ğ‘ğ‘ğ‘ğ‘ ğ‘ ğ‘™ğ‘™â„ ğ‘ğ‘ğ‘’ğ‘’ğ‘ ğ‘ ) = 2/9 = 0.222
ğ‘ƒğ‘ƒ ğ‘šğ‘šğ‘›ğ‘›ğ‘ğ‘ğ‘ğ‘ğ‘šğ‘šğ‘’ğ‘’ = ğ‘šğ‘šğ‘’ğ‘’ğ‘’ğ‘’ğ‘šğ‘šğ‘ ğ‘ ğ‘šğ‘š ğ‘ğ‘ğ‘’ğ‘’ğ‘ ğ‘ ) = 4/9 = 0.444
ğ‘ƒğ‘ƒ ğ‘ ğ‘ ğ‘™ğ‘™ğ‘ ğ‘ ğ‘’ğ‘’ğ‘’ğ‘’ğ‘›ğ‘›ğ‘™ğ‘™ = ğ‘ğ‘ğ‘’ğ‘’ğ‘ ğ‘  ğ‘ğ‘ğ‘’ğ‘’ğ‘ ğ‘ ) = 6/9 = 0.667
ğ‘ƒğ‘ƒ ğ‘ğ‘ğ‘ğ‘ğ‘’ğ‘’ğ‘’ğ‘’ğ‘šğ‘šğ‘™ğ‘™_ğ‘ğ‘ğ‘’ğ‘’ğ‘™ğ‘™ğ‘šğ‘šğ‘›ğ‘›ğ‘Ÿğ‘Ÿ = ğ‘ğ‘ğ‘’ğ‘’ğ‘šğ‘šğ‘ğ‘ ğ‘ğ‘ğ‘’ğ‘’ğ‘ ğ‘ ) = 6/9 = 0.667
ğ‘ƒğ‘ƒ ğ‘‹ğ‘‹ ğ‘ğ‘ğ‘ ğ‘ ğ‘ğ‘_ğ‘ğ‘ğ‘ğ‘ğ‘šğ‘šğ‘ğ‘ğ‘ ğ‘ ğ‘™ğ‘™ğ‘’ğ‘’ğ‘ğ‘ = ğ‘ğ‘ğ‘’ğ‘’ğ‘ ğ‘ ) = 0.044

ğ‘ƒğ‘ƒ ğ‘‹ğ‘‹ ğ‘ğ‘ğ‘ ğ‘ ğ‘ğ‘_ğ‘ğ‘ğ‘ğ‘ğ‘šğ‘šğ‘ğ‘ğ‘ ğ‘ ğ‘™ğ‘™ğ‘’ğ‘’ğ‘ğ‘ = ğ‘›ğ‘›ğ‘ğ‘) = 0.019

Through Bayesian:
â€¢
â€¢
Conclusion: X will buy a computer.

ğ‘ƒğ‘ƒ ğ‘‹ğ‘‹ | ğ‘ğ‘ğ‘’ğ‘’ğ‘ ğ‘  Ã— ğ‘ƒğ‘ƒ ğ‘ğ‘ğ‘ ğ‘ ğ‘ğ‘_ğ‘ğ‘ğ‘ğ‘ğ‘šğ‘šğ‘ğ‘ğ‘ ğ‘ ğ‘™ğ‘™ğ‘’ğ‘’ğ‘ğ‘ = ğ‘ğ‘ğ‘’ğ‘’ğ‘ ğ‘  = 0.028
ğ‘ƒğ‘ƒ ğ‘‹ğ‘‹ | ğ‘›ğ‘›ğ‘ğ‘ Ã— ğ‘ƒğ‘ƒ ğ‘ğ‘ğ‘ ğ‘ ğ‘ğ‘_ğ‘ğ‘ğ‘ğ‘ğ‘šğ‘šğ‘ğ‘ğ‘ ğ‘ ğ‘™ğ‘™ğ‘’ğ‘’ğ‘ğ‘ = ğ‘›ğ‘›ğ‘ğ‘ = 0.007

47

Evaluation Measures

ï¯ To assess how â€œaccurateâ€ your classifier is at predicting the 

class label of tuples compared to actual labels
ï® True Positives TP: positive tuples that were correctly labeled

ï¯ Positive tuples: tuples of the main class of interest

ï® True Negatives TN: negative tuples that were correctly labeled 
ï® False Positives FP: negative tuples that were incorrectly labeled 

as positive (e.g., people who do not buy computers but are 
labeled as 

)

ğ‘ğ‘ğ‘ ğ‘ ğ‘ğ‘ğ‘ ğ‘ _ğ‘ğ‘ğ‘ğ‘ğ‘šğ‘šğ‘ğ‘ğ‘ ğ‘ ğ‘™ğ‘™ğ‘’ğ‘’ğ‘ğ‘ = ğ‘ğ‘ğ‘’ğ‘’ğ‘ ğ‘ 

ï® False Negatives FN: positive 
tuples that were mislabeled as 
negative (e.g., people who really 
buy computers but are labeled 
as 

)

ğ‘ğ‘ğ‘ ğ‘ ğ‘ğ‘ğ‘ ğ‘ _ğ‘ğ‘ğ‘ğ‘ğ‘šğ‘šğ‘ğ‘ğ‘ ğ‘ ğ‘™ğ‘™ğ‘’ğ‘’ğ‘ğ‘ = ğ‘›ğ‘›ğ‘ğ‘

48

Evaluation Measures

49

CLUSTERING

50

Partitioning Algorithms: Basic Concepts

ï¯ Partitioning method

ï® Discover groupings in the data by optimizing a specific objective 

function and iteratively improving the quality of partitions
-partitioning method

ï¯

ï® Objective: Divide a dataset 
ğ‘²ğ‘²

of 

objects into a set of 

clusters, 

so that an objective function is optimized (e.g., minimizing the sum 
ğ‘›ğ‘›
of distances within clusters)

ğ‘«ğ‘«
ï® Typical objective function: Sum of Squared Errors (SSE)

ğ¾ğ¾

SSE C

(

)

=

K

âˆ‘ âˆ‘

k

=
1

x
âˆˆ
i Ck

||

x
i

âˆ’
c
k

2
||

where 

is the centroid or medoid of cluster 

ğ‘ğ‘ğ‘˜ğ‘˜

ğ¶ğ¶ğ‘˜ğ‘˜

51

The 

-Means Clustering Algorithm 

ğ¾ğ¾

ï¯ Idea: each cluster is represented by the centroid, which is the 

mean position of all data points in the cluster
ï® It may not correspond to an actual data point in the dataset!

ï¯ Given 

, the number of clusters, the 

-Means clustering 

algorithm is outlined as follows:

ğ¾ğ¾
Initialization: Select 
Repeat

data points as initial centroids

ğ¾ğ¾

clusters by assigning each point to its closest centroid

â€¢ Form 
â€¢ Re-compute the centroids (i.e., mean point) of each cluster
Until centroids no longer change or convergence criterion is met

ğ¾ğ¾

ğ¾ğ¾

52

Discussion on 

-Means Clustering (I)

ï¯ Limitations

ğ¾ğ¾
ï® Need to specify 

in advance 
ï¯ There are ways to automatically determine the â€˜bestâ€™
ï¯ In practice, one often runs a range of values and selected the â€˜bestâ€™.

ğ¾ğ¾

.

ğ¾ğ¾

ï® Only for objects in a continuous data space: 

-modes for 

nominal data

ï®

-means clustering often terminates at a local optimum.

ğ¾ğ¾

ï¯ Poor initialization can lead to suboptimal clusters.
ğ¾ğ¾
ï® Sensitive to noisy data and outliers (extreme values)

53

Measuring Clustering Quality

ï¯ Evaluation: Evaluating the goodness of clustering results

ï® No universally recognized â€˜bestâ€™ measure in practice!

ï¯ Three categorization of measures

ï® Internal: Unsupervised, criteria derived from data itself

ï¯ How well the clusters are separated and how compact the clusters are
ï® External: Supervised, employ criteria not inherent to the dataset
ï¯ Compare a clustering against prior or expert-specified knowledge (i.e., 

the ground truth) using certain clustering quality measures
ï® Relative: Directly compare different clustering, usually those 

obtained by varying parameters for the same algorithm

54

Email: fengmei.jin@polyu.edu.hk

Office: PQ747

THANK YOU!

56

