COMP5121
Data Mining and Data Warehousing Applications

Week 5: Mining Frequent Patterns, Association and Correlations: 
Basic Concepts and Methods

Dr. Fengmei Jin
ï‚§ Email: fengmei.jin@polyu.edu.hk
ï‚§ Office: PQ747 (+852 3400 3327)
ï‚§ Consultation Hours: 2.30-4.30 pm every Thursday

Outline

ï¯ Basic Concepts

ï® Pattern discovery, frequent itemsets, association rules, support, 

confidence, closed patterns, maximal patterns

ï¯ Frequent Itemset Mining Methods 
ï® Apriori: downward closure property
ï® Other more efficient methods

ï¯ Pattern Evaluation Methods â€“ Interestingness

ï® Support-Confidence framework
ï® Lift and Chi-Square
ï® Null-invariant measures

3

What Is Pattern Discovery?

ï¯ To identify meaningful relationships, trends, structures, etc.

ï¯ Frequent Pattern Mining: a key method in pattern discovery
ï® Search for patterns that occur frequently in massive data

ï¯ Frequent occurrences of items (e.g., products purchased together)
ï¯ Sequential patterns (e.g., events or actions happening in a sequence)
ï¯ Structured patterns (e.g., subgraphs in networks)

4

What Is Pattern Discovery?

ï¯ Motivation examples in real world

Purchased 
together

ï® What products were often purchased together?
ï® What combinations of symptoms frequently co-occur among patients?

ï® What are the subsequent purchases after buying an iPad?
ï® What word sequences likely form phrases in a corpus?

ï® What kinds of network structures indicate influential groups in Twitter?
ï® What spatial patterns in road networks imply traffic congestion?

5

Why Is â€˜Frequent Patternâ€™ Important?

ï¯ Revealing inherent regularities and important properties of data
ï¯ Foundation for many essential data mining tasks:

ï® Association, correlation, and causality analysis
ï® Classification: Discriminative pattern-based analysis
ï® Cluster analysis: Pattern-based subspace clustering
ï® Other various patterns in spatiotemporal, multimedia, time-series data

ï¯ Broad applications:

ï® E-commerce: market-basket analysis, cross-marketing, catalog design
ï® Management: sale campaign analysis, Web log analysis
ï® Healthcare: biological sequence analysis

6

Market Basket Analysis: A Motivating Example

ï¯ â€œWhich groups or sets of items are customers likely to 

purchase on a given trip to the store?â€

ï® Finding frequent itemsets

ï® Items frequently purchased 

together by customers

ï® Benefits: 

ï® Design store layouts, 
ï® Understand buying habits
ï® Plan marketing / advertising 

strategies

7

Basic Concepts: Frequent Itemsets

ï¯ Itemset: A set of one or more items

ï®

-itemset: 
ï¯ Support of an itemset

ğ’Œğ’Œ

ğ‘‹ğ‘‹ = ğ‘¥ğ‘¥1, â€¦ , ğ‘¥ğ‘¥ğ‘˜ğ‘˜

ğ‘˜ğ‘˜

with 

items

ï® Absolute Support (Count): the number of transactions 

containing the given itemset 

ï® Relative Support: the fraction of transactions containing 

(i.e., 

the probability that a transaction contains 

ğ‘‹ğ‘‹

)

ğ‘‹ğ‘‹

ï¯ Frequent Itemset: An itemset 

ğ‘‹ğ‘‹

is frequent if the support of 

is no less than 

â€“ a minsup threshold.

ğ‘‹ğ‘‹

ğœğœ

ğ‘‹ğ‘‹

8

Basic Concepts: Frequent Itemsets

ï¯ Let minsup = 50% 
ï¯ Frequent 1-itemsets:
ï® Beer: 3 (60%)
ï® Nuts: 3 (60%)
ï® Eggs: 3 (60%)
ï® Diaper: 4 (80%)
ï¯ Frequent 2-itemsets: 

ï® {Beer, Diaper}: 3 (60%)

Tid

10

20

30

40

50

Items bought

Beer, Nuts, Diaper

Beer, Coffee, Diaper

Beer, Diaper, Eggs

Nuts, Eggs, Milk

Nuts, Coffee, Diaper, Eggs, Milk

9

From Frequent Itemsets to Association Rules

ï¯ Association Rules written as X ïƒ  Y [support, confidence]

ï® Both 
are non-empty itemsets, and 
ï® It describes an â€˜if-thenâ€™ relationship between two sets of items.

and 

.

ğ‘‹ğ‘‹

ğ‘Œğ‘Œ

ğ‘‹ğ‘‹ âˆ© ğ‘Œğ‘Œ = âˆ…

ï® Support: The percentage of transactions containing both X and Y

ï¯

: the percentage of transactions that contains every item in 

, i.e., how frequently both 

appear together in the dataset
ğ‘‹ğ‘‹
ï® Confidence: The conditional probability that a transaction having 
ğ‘‹ğ‘‹

and 
ğ‘ƒğ‘ƒ(ğ‘‹ğ‘‹ âˆª ğ‘Œğ‘Œ)
ğ‘Œğ‘Œ

ğ‘Œğ‘Œ

sup(ğ‘‹ğ‘‹ â†’ ğ‘Œğ‘Œ) = P(ğ‘‹ğ‘‹ âˆª ğ‘Œğ‘Œ)
and 

also contains 

, that is,

ğ‘‹ğ‘‹

ğ‘Œğ‘Œ

conf ğ‘‹ğ‘‹ â†’ ğ‘Œğ‘Œ = P ğ‘Œğ‘Œ ğ‘‹ğ‘‹ = sup(ğ‘‹ğ‘‹ â†’ ğ‘Œğ‘Œ)/sup(ğ‘‹ğ‘‹)

10

Association Rule Mining

ï¯ Strong Rules: Find all rules, 

that satisfy

ï® minimum support: frequency of X and Y appear together.
ï® minimum confidence: likelihood that Y occurs when X occurs.

ğ‘‹ğ‘‹ â†’ ğ‘Œğ‘Œ

ï¯ What does a strong rule do really?

ï® It correlates the presence of one set of items with another set.
ï® Applications:
â†’
â†’ 
ğ’€ğ’€
âˆ—

: What actions can boost sales of 
: What other products should be stocked up if 

is popular?

ï¯
ï¯

âˆ—
ğ‘¿ğ‘¿

ğ‘‹ğ‘‹

ğ‘Œğ‘Œ

?

11

Association Rule Mining: An Example

ï¯ Frequent itemsets: Let minsup = 50% 

ï® Freq. 1-itemset: Beer: 3; Nuts: 3; Eggs: 3; Diaper: 4
ï® Freq. 2-itemsets:  {Beer, Diaper}: 3
ï¯ Association rules:  Let minconf = 50%

ï® Beer ïƒ  Diaper  (60%, 100%)
ï® Diaper ïƒ  Beer  (60%, 75%)

Customers 
buy both

Tid

10

20

30

40

50

Items bought

Beer, Nuts, Diaper

Beer, Coffee, Diaper

Beer, Diaper, Eggs

Nuts, Eggs, Milk

Nuts, Coffee, Diaper, Eggs, Milk

Beer

Diaper

Customers 
buy beer

Customers 
buy diaper

12

Challenge: Too Many Frequent Patterns!

ï¯ Long patterns generate an exponential number of sub-patterns.
ï¯ Given two transactions with minimum support = 1:

ï®
ï® How many frequent itemsets?
, â€¦, 

ğ‘»ğ‘»ğŸğŸ: ğ’‚ğ’‚ğŸğŸ, â€¦ , ğ’‚ğ’‚ğŸğŸğŸ“ğŸ“ğŸ“ğŸ“

; 

, 

. 

, 

, â€¦, 

ğ‘»ğ‘»ğŸğŸ: ğ’‚ğ’‚ğŸğŸ, â€¦ , ğ’‚ğ’‚ğŸ“ğŸ“ğŸ“ğŸ“
ï¯ 1-itemsets: 
ï¯ 2-itemsets: 
ï¯ â€¦, â€¦, â€¦, â€¦
ï¯ 99-itemsets: 
ï¯ 100-itemsets: 

ï® In total: 

, â€¦, 
ğ‘ğ‘1 : 2

ğ‘ğ‘1 : 2
ğ‘ğ‘1, ğ‘ğ‘2 : 2

, 
ğ‘ğ‘50 : 2
ğ‘ğ‘1, ğ‘ğ‘50 : 2
, â€¦, 

ğ‘ğ‘51 : 1
ğ‘ğ‘1, ğ‘ğ‘51 : 1

, â€¦, â€¦, 

ğ‘ğ‘100 : 1

ğ‘ğ‘99, ğ‘ğ‘100 : 1
Too huge for any 
computer to 
compute or store!

ğ‘ğ‘1, ğ‘ğ‘2, â€¦ , ğ‘ğ‘99 : 1
ğ‘ğ‘1, ğ‘ğ‘2, â€¦ , ğ‘ğ‘100 : 1
100
100
100 = 2
2 + â‹¯ +

100

100
1 +

ğ‘ğ‘2, ğ‘ğ‘3, â€¦ , ğ‘ğ‘100 : 1

sub-patterns!

â€“ 1

13

Expressing Patterns in Compressed Form: Closed Patterns

ï¯ How to handle such a scalability challenge?
ï¯ Solution 1: Closed patterns

An itemset 
is frequent
â€¢
ğ‘¿ğ‘¿
â€¢ NO super-pattern 

is a closed pattern if:

exists with the same support as 

.

ğ‘‹ğ‘‹
ï® Given 

; 
ğ‘Œğ‘Œ âŠƒ ğ‘‹ğ‘‹

with minsup = 1
ğ‘‹ğ‘‹

ï¯ Only two closed patterns ïƒ 
ğ‘‡ğ‘‡1: ğ‘ğ‘1, â€¦ , ğ‘ğ‘50

ğ‘‡ğ‘‡2: ğ‘ğ‘1, â€¦ , ğ‘ğ‘100

, 

ï¯ Closed pattern is a lossless compression of frequent patterns.

ğ’‚ğ’‚ğŸğŸ, â€¦ , ğ’‚ğ’‚ğŸ“ğŸ“ğŸ“ğŸ“ : ğŸğŸ

ğ’‚ğ’‚ğŸğŸ, â€¦ , ğ’‚ğ’‚ğŸğŸğŸ“ğŸ“ğŸ“ğŸ“ : ğŸğŸ

ï® Reduces # patterns to process
ï® Retains all support information: â€œ

â€, â€œ

â€, â€¦

14

ğ‘ğ‘2, â€¦ , ğ‘ğ‘40 : 2

ğ‘ğ‘5, â€¦ , ğ‘ğ‘51 : 1

Expressing Patterns in Compressed Form: Maximal Patterns

ï¯ Solution 2: Max-patterns  

is a maximal pattern if:

A pattern 
â€¢
â€¢ NO frequent super-pattern 

is frequent

ğ‘‹ğ‘‹

exist.

ğ‘‹ğ‘‹

ï® Given 

; 

ğ‘Œğ‘Œ âŠƒ ğ‘‹ğ‘‹
with minsup = 1

ï¯ Only one max-pattern ïƒ 
ğ‘‡ğ‘‡1: ğ‘ğ‘1, â€¦ , ğ‘ğ‘50

ğ‘‡ğ‘‡2: ğ‘ğ‘1, â€¦ , ğ‘ğ‘100
ğ‘ğ‘1, â€¦ , ğ‘ğ‘100 : 1

ï¯ Limitation: Maximal patterns are lossy compression! 

ï® Compared to close patterns, this method does NOT reveal the 

real support for sub-patterns of a max-pattern.

ï® Example: We only know

is frequent, but cannot know 

its real support.

ğ‘ğ‘1, â€¦ , ğ‘ğ‘40
Therefore, closed patterns is more desirable than maximal patterns.

15

Apriori, FPGrowth, strong association rules, â€¦

FREQUENT ITEMSET MINING METHODS 

16

A Frequent Pattern Implies Frequent Subsets

ï¯ Key Observation

ï® Given 

and 

, we get a frequent 

itemset: 

ğ‘‡ğ‘‡1: ğ‘ğ‘1, â€¦ , ğ‘ğ‘50
ï® All subsets are also frequent: 
ğ‘ğ‘1, â€¦ , ğ‘ğ‘50
, â€¦

ğ‘‡ğ‘‡2: ğ‘ğ‘1, â€¦ , ğ‘ğ‘100
, 

, â€¦, 

, 

, â€¦, 

ğ‘ğ‘50
ğ‘ğ‘1
There must be some hidden relationships 
among these frequent patterns! 

ğ‘ğ‘1, ğ‘ğ‘2

ğ‘ğ‘2

ğ‘ğ‘1, â€¦ , ğ‘ğ‘49

The Downward Closure Property in Apriori

ï¯ Any subset of a frequent itemset must be frequent!
ï® e.g., if {beer, diaper, nuts} is frequent, so is {beer, diaper}

ï¯ Reason: Every transaction containing {beer, diaper, nuts} also contains 

{beer, diaper}. 

ï® Pruning strategy: If any subset of an itemset 

ï¯ Apriori: an efficient mining algorithm with downward closure.
is infrequent, 
at all?
cannot be frequent. In this case, why consider 
ï® Eliminate infrequent itemsets early. Focus on promising ones.

then 

ğ‘†ğ‘†

ğ‘†ğ‘†

ğ‘†ğ‘†

Apriori Pruning and Scalable Mining Methods

ï¯ Aprioriâ€™s pruning principle

ï® If any itemset is infrequent, its superset should not even be generated! 
ï® Example: If {A, B} is infrequent, {A, B, C} will not be considered.

ï¯ Scalable Mining Methods

ï® Apriori: Level-wise, join-based approach (Agrawal, et al. @VLDBâ€™94)

ï¯ Iterative procedure: 

-itemsets are used to explore 

-itemsets.

ï® Eclat: Use vertical data format to compute intersections of transactions 

ğ‘˜ğ‘˜ + 1

ğ‘˜ğ‘˜
(Zaki, et al. @KDDâ€™97)

ï® FP-growth: Avoid candidate generation by building a compact FP-tree 

(Han, et al. @SIGMODâ€™00)

19

The Apriori Algorithm: Framework

ï¯ Outline of Apriori: level-wise, candidate generation and test 

ïƒ˜ Initially, scan DB once to get frequent 1-itemset
ïƒ˜ Repeat

â€¢ Generate length-
â€¢ Test the candidates against DB to find frequent 
â€¢ Set 

ğ‘˜ğ‘˜ + 1

candidate itemsets based on frequent 

-itemsets

-itemsets
ğ‘˜ğ‘˜

ğ‘˜ğ‘˜ + 1

ïƒ˜ Until no frequent or candidate set can be generated
ïƒ˜ Return all the frequent itemsets derived

ğ‘˜ğ‘˜ â‰” ğ‘˜ğ‘˜ + 1

20

The Apriori Algorithm: Pseudo-Code

ï¯

ï¯

: Candidate itemsets of size 

: Frequent itemsets of size 

ğ¶ğ¶ğ‘˜ğ‘˜
ğ¹ğ¹ğ‘˜ğ‘˜
;
{frequent items}; 

) Do {

ğ‘˜ğ‘˜ = 1
While (
ğ¹ğ¹ğ‘˜ğ‘˜ =

= candidates generated from 

ğ¹ğ¹ğ‘˜ğ‘˜ â‰  âˆ…

Derive 
ğ¶ğ¶ğ‘˜ğ‘˜+1

by counting candidates in 

ğ¹ğ¹ğ‘˜ğ‘˜

ğ¶ğ¶ğ‘˜ğ‘˜+1

;
ğ¹ğ¹ğ‘˜ğ‘˜+1

}

ğ‘˜ğ‘˜ = ğ‘˜ğ‘˜ + 1

ğ‘˜ğ‘˜

// frequent 1-itemset

// as long as 

is non-empty

ğ¹ğ¹ğ‘˜ğ‘˜
w.r.t. TDB at minsup;

// test candi

// candidate generation

ğ‘˜ğ‘˜

;  

Return

;

// return 

generated at each level

âˆªğ‘˜ğ‘˜ ğ¹ğ¹ğ‘˜ğ‘˜

ğ¹ğ¹ğ‘˜ğ‘˜

21

The Apriori Algorithm: An Example 

minsup = 2

Itemset

sup

Database TDB

Tid

10

20

30

40

F2

Items

A, C, D

B, C, E

A, B, C, E

B, E

C1

1st scan

C2

Itemset
{A, C}
{B, C}
{B, E}
{C, E}

sup
2
2
3
2

{A}

{B}

{C}

{D}

{E}

2

3

3

1

3

Itemset
{A, B}
{A, C}
{A, E}
{B, C}
{B, E}
{C, E}

sup
1
2
1
2
3
2

F1

Itemset

sup

{A}

{B}

{C}

{E}

2

3

3

3

C2

Itemset

2nd scan

{A, B}

{A, C}

{A, E}

{B, C}

{B, E}

{C, E}

C3

Itemset

{B, C, E}

3rd scan

F3

Itemset

{B, C, E}

sup

2

22

Candidate Generation in Apriori

ï¯ Efficiently generate candidates

ï® Step 1: self-join

to generate 

candidates of size 
ğ¹ğ¹ğ‘˜ğ‘˜

ï® Step 2: prune candidates whose 

subsets are not all frequent

ğ‘˜ğ‘˜ + 1

ï¯ Example
ï® Input: 
ï® Self-joining: 
ï® Pruning: 
ï® Output: 

ïƒ 

, 

ğ¹ğ¹3 = ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘
ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘

is removed because 
ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘

ğ¹ğ¹3 Ã— ğ¹ğ¹3

ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘
ğ¶ğ¶4 = ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘

self-join

self-join

abc

abd

acd

ace

bcd

abcd

acde

pruned

is NOT in 

ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘

ğ¹ğ¹3

23

Candidate Generation: An SQL Implementation

ï¯ Suppose the items in 

are listed in an order (e.g., bac ïƒ  abc)

ğ¹ğ¹ğ‘˜ğ‘˜âˆ’1

ğ¹ğ¹ğ‘˜ğ‘˜âˆ’1

, 

, â€¦, 

, 

Step 1: self-joining 

insert into 

select 

from 

where 

ğ‘ªğ‘ªğ’Œğ’Œ
as 
ğ’‘ğ’‘. ğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’ğ’ğŸğŸ
ğ‘­ğ‘­ğ’Œğ’Œâˆ’ğŸğŸ

as 
, 
ğ’‘ğ’‘. ğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’ğ’ğŸğŸ
ğ‘­ğ‘­ğ’Œğ’Œâˆ’ğŸğŸ

ğ’‘ğ’‘

ğ’’ğ’’
ğ’‘ğ’‘. ğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’ğ’ğŸğŸ = ğ’’ğ’’. ğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’ğ’ğŸğŸ

Step 2: pruning

for all itemsets 

in 

do

ğ’‘ğ’‘. ğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’ğ’ğ’Œğ’Œâˆ’ğŸğŸ

ğ’’ğ’’_ğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’ğ’ğ’Œğ’Œâˆ’ğŸğŸ

, â€¦, 

, 

for all 

-subsets 
ğ‘ªğ‘ªğ’Œğ’Œ
ğ’„ğ’„

of 

do

if (

is not in 
(ğ‘˜ğ‘˜ âˆ’ 1)

) then delete 
ğ’„ğ’„

ğ’”ğ’”

from 

ğ’”ğ’”

ğ‘­ğ‘­ğ’Œğ’Œâˆ’ğŸğŸ

ğ’„ğ’„

ğ‘ªğ‘ªğ’Œğ’Œ

ğ’‘ğ’‘. ğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’ğ’ğ’Œğ’Œâˆ’ğŸğŸ = ğ’’ğ’’. ğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’ğ’ğ’Œğ’Œâˆ’ğŸğŸ

ğ’‘ğ’‘. ğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’ğ’ğ’Œğ’Œâˆ’ğŸğŸ < ğ’’ğ’’. ğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’ğ’ğ’Œğ’Œâˆ’ğŸğŸ

self-join

self-join

abc

abd

acd

ace

bcd

abcd

acde

pruned

24

Final Step: Rule Generation via Frequent Itemsets

ï¯ Support (min-sup): used to mine the frequent itemsets
ï¯ Confidence (min-conf): used by the rule generation step to 

qualify the strength of the derived association rules
ï® For each frequent itemset 
ï® For every non-empty subset 
ğ¹ğ¹

, generate a rule:
ğ¹ğ¹

, generate 

â€™s all non-empty subsets

satisfies the minimum confidence, i.e., 

ğ‘ ğ‘ 

ğ‘…ğ‘…: ğ‘ ğ‘  â†’ ğ¹ğ¹ âˆ’ ğ‘ ğ‘ 

is a strong association rule and should be output.

conf ğ‘ ğ‘  â†’ ğ¹ğ¹ âˆ’ ğ‘ ğ‘  =

sup ğ¹ğ¹
sup ğ‘ ğ‘  â‰¥ ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š _ğ‘ğ‘ğ‘ğ‘ğ‘šğ‘šğ‘ğ‘

ï® If the rule 

ğ‘…ğ‘…

then 

ğ‘…ğ‘…

25

Rule Generation: An Example

ï¯ For 
ï®

, 

, 
, 
ğ¹ğ¹3 = 2,3,5
3
2

5

, there are six non-empty subsets:

, 

, 

ï¯ Thus, six candidate rules can be generated

2,3

2,5

3,5

ï®
ï®

, 
, 

, 
, 

2 â†’ 3,5
2,3 â†’ 5

3 â†’ 2,5
2,5 â†’ 3

5 â†’ 2,5
3,5 â†’ 2

ï¯ If any of them satisfies the minimum confidence, it will be 

output to the end user.

26

Is Apriori Fast Enough? 

ï¯ Core of the Apriori algorithm:

ï® Use frequent 
ï® Use scanning and pattern matching to calculate support for candidates

-itemsets to generate candidate 

-itemsets

ğ‘˜ğ‘˜

(ğ‘˜ğ‘˜ + 1)

ï¯ The performance bottleneck of Apriori: candidate generation

ï® Huge candidate sets (exponential growth)
frequent 1-itemset will generate 

ï¯
ï¯ To discover a frequent pattern of size 100, e.g., {a1, a2, â€¦, a100}, one 

candidate 2-itemsets

4
10
needs to generate 

7
> 10

candidates.

30
ï® Multiple scans of database
â‰ˆ 10

100

2
scans, where 

ï¯ Needs 

is the length of the longest pattern

ğ‘šğ‘š + 1

ğ‘šğ‘š

27

*Techniques to Enhance Aprioriâ€™s Efficiency

ï¯ Shrink # candidates

ï® Hashing: A 
-itemset whose hashing bucket count < threshold cannot be frequent
ï® Sampling: mining on a subset of data with lower min-sup to ensure completeness

ğ‘˜ğ‘˜
ï¯ Reduce database scans

ï® Partitioning: Any itemset that is potentially frequent in DB must be frequent in at least 

one of the partitions of DB

ï® Dynamic itemset counting: Adding new candidate itemsets only when all of their 

subsets are estimated to be frequent

ï® Transaction reduction: A transaction that does not contain any frequent 

-itemset is 

useless in subsequent scans
ï¯ Explore special data structures

ï® Tree projection, H-miner, Hypercube decomposition

ğ‘˜ğ‘˜

28

*Partitioning: Scan Database Only Twice

ï¯ Theorem: Any itemset that is potentially frequent in TDB must 

be frequent in at least one of the partitions of TDB
Support threshold: 
Partition: 

ğœğœ

TDB1 + TDB2 + â‹¯ + TDBk = TDB

TDB1

+

TDB2

+

. . .

. . .

+

TDBk

=       TDB

sup1 ğ‘‹ğ‘‹ < ğœğœ TDB1

ï¯ Method [A. Savasere, E. Omiecinski, S. Navathe, VLDBâ€™95]

sup ğ‘‹ğ‘‹ < ğœğœ TDB

supğ‘˜ğ‘˜ ğ‘‹ğ‘‹ < ğœğœ TDBğ‘˜ğ‘˜

sup2 ğ‘‹ğ‘‹ < ğœğœ TDB2

ï® Scan 1: Partition database and find local frequent patterns
ï® Scan 2: Consolidate global frequent patterns

29

*DHP: Direct Hashing and Pruning

ï¯ Reduce # candidates  [J. Park, M. Chen, P. Yu, SIGMODâ€™95]
ï¯ Observation: A 

-itemset whose corresponding hashing 

bucket count is below the threshold cannot be frequent
ğ‘˜ğ‘˜
ï® Frequent items: 
ï¯ minsup = 2

Items

a, b

Tid

1

Itemsets

{ab, ad, ae}

2

3

4

a, b, d, e

{bd, be, de}

a, e, f

c, d

Hash Table

Count

3

1

ï® Candidate 2-itemsets:

ğ‘ğ‘, ğ‘ğ‘, ğ‘ğ‘, ğ‘ğ‘

ï® Hash buckets with counts
ğ‘ğ‘ğ‘ğ‘, ğ‘ğ‘ğ‘ğ‘, ğ‘ğ‘ğ‘ğ‘, ğ‘ğ‘ğ‘ğ‘, ğ‘ğ‘ğ‘ğ‘, ğ‘ğ‘ğ‘ğ‘

ï¯

ï¯

is not a promising candidate 2-itemset as the count of 

is below the support threshold

{ğ‘ğ‘ğ‘ğ‘}
{ğ‘ğ‘ğ‘ğ‘, ğ‘ğ‘ğ‘ğ‘, ğ‘ğ‘ğ‘ğ‘}

30

*ECLAT: Exploring Vertical Data Format

ï¯ A depth-first search algorithm using set intersection [Zaki et al. @KDDâ€™97]
ï¯ Tid-List: List of transaction-ids containing an itemset 
ïƒ 

ï® Vertical format: 
ï® Deriving frequent patterns based on vertical intersections

; 

ğ‘¡ğ‘¡ ğ‘ğ‘ = ğ‘‡ğ‘‡10, ğ‘‡ğ‘‡20

ğ‘¡ğ‘¡ ğ‘ğ‘ = ğ‘‡ğ‘‡10, ğ‘‡ğ‘‡20, ğ‘‡ğ‘‡30

ğ‘¡ğ‘¡ ğ‘ğ‘ğ‘ğ‘ = ğ‘‡ğ‘‡10, ğ‘‡ğ‘‡20

A transaction DB in 
Horizontal Data Format

The transaction DB in 
Vertical Data Format

Tid

Itemset

Item

TidList

always occur together in the same 

and 

Properties of Tid-Lists
â€¢

If 
transactions, then 
ğ‘Œğ‘Œ
If 
ğ‘¡ğ‘¡(ğ‘‹ğ‘‹) = ğ‘¡ğ‘¡(ğ‘Œğ‘Œ)
Y must have X. For example, 

, then 

ğ‘‹ğ‘‹

â€¢

. e.g., 

.

, as transactions having 

ğ‘¡ğ‘¡ ğ‘ğ‘ğ‘ğ‘ = ğ‘¡ğ‘¡ ğ‘ğ‘
.

ğ‘¡ğ‘¡ ğ‘Œğ‘Œ âŠ† ğ‘¡ğ‘¡ ğ‘‹ğ‘‹

ğ‘‹ğ‘‹ âŠ† ğ‘Œğ‘Œ
ï¯ Using diffset to accelerate mining

ğ‘¡ğ‘¡ ğ‘ğ‘ğ‘ğ‘ âŠ† ğ‘¡ğ‘¡ ğ‘ğ‘

10

20

30

a, c, d, e

a, b, e

b, c, e

ï® Only keep track of differences of Tids
ï®

, 

Diffset

ğ‘¡ğ‘¡(ğ‘ğ‘) = ğ‘‡ğ‘‡10, ğ‘‡ğ‘‡20, ğ‘‡ğ‘‡30

ğ‘¡ğ‘¡ ğ‘ğ‘ğ‘ğ‘ = ğ‘‡ğ‘‡10, ğ‘‡ğ‘‡30 â†’

(ğ‘ğ‘ğ‘ğ‘, ğ‘ğ‘) = ğ‘‡ğ‘‡20

a

b

c

d

e

10, 20

20, 30

10, 30

10

10, 20, 30

31

pattern evaluation methods

WHICH PATTERNS ARE INTERESTING?

32

How to Evaluate if a Rule/Pattern Is Interesting?

ï¯ Pattern-mining will generate a large set of patterns/rules.
ï¯ Not all the generated patterns/rules are interesting.

ï¯ Interestingness Measures

ï® Objective: based on statistics behind the data

ï¯ Support, confidence, correlation, â€¦

ï® Subjective: â€œone manâ€™s trash could be otherâ€™s treasureâ€

ï¯ Query-based: Relevant to a userâ€™s particular request (actionable)
ï¯ Against oneâ€™s knowledge-base: unexpected, freshness, timeliness
ï¯ Visualization tools: Multi-dimensional, interactive examination

33

Limitation of the Support-Confidence Framework

ï¯ Strong rules are not necessarily interesting: â€œ
ï¯ Example: Suppose a school may have the following statistics 

â€ 

on # students related to playing basketball and/or eating cereal:

ğ´ğ´ â†’ ğµğµ

ğ‘ ğ‘ , ğ‘ğ‘

Play basketball

Not play basketball

Eat cereal

Not eat cereal

sum

400

200

600

350

50

400

sum

750

250

1000 (TOTAL)

ï® Association rule mining may generate a rule:

play-basketball ïƒ  eat-cereal [40%, 66.7%]

ï® But this strong association rule is misleading ïƒ  The overall % of 

students eating cereal is 75% > 66.7%. 

ï® A more telling rule: 

not play-basketball ïƒ  eat-cereal [35%, 87.5%] (high 

& 

)

34

ğ‘ ğ‘ 

ğ‘ğ‘

Interestingness Measure: Lift

ï¯ Measure of dependent / correlated events:

ï® Tell how 
ğ’ğ’ğ’Šğ’Šğ’ğ’ğ’Šğ’Š ğ‘©ğ‘©, ğ‘ªğ‘ª =

ï¯
ï¯
ï¯

ğ‘™ğ‘™ğ‘šğ‘šğ‘ğ‘ğ‘¡ğ‘¡ ğµğµ, ğ¶ğ¶ = 1
ğ‘™ğ‘™ğ‘šğ‘šğ‘ğ‘ğ‘¡ğ‘¡ ğµğµ, ğ¶ğ¶ > 1
ğ‘™ğ‘™ğ‘šğ‘šğ‘ğ‘ğ‘¡ğ‘¡ ğµğµ, ğ¶ğ¶ < 1

ï® Example:
=CB
)

,

lift

(

600
/
ï¯ Thus, 
ï¯
and 

P ğµğµ âˆª ğ¶ğ¶
and 
ğ‘ƒğ‘ƒ ğµğµ ğ‘ƒğ‘ƒ ğ¶ğ¶
: 
: positively correlated
: negatively correlated

sup(ğµğµ â†’ ğ¶ğ¶)
=
are correlated
sup ğµğµ sup(ğ¶ğ¶)
and 
ğ¶ğ¶
ğµğµ

=
are independent

ğµğµ

ğ¶ğ¶

conf ğµğµ â†’ ğ¶ğ¶
sup ğ¶ğ¶
C
Not C

400

200

sum

600

B

Not B

sum

350

50

400

750

250

1000

is more telling than s & c

400
1000

/
1000
Ã—
750
and 

=

89.0

lift

(

/

1000

=Â¬CB

)

,

600

/

ğ’ğ’ğ’Šğ’Šğ’ğ’ğ’Šğ’Š

are negatively correlated since 

200
1000

1000
/
Ã—
250
.

=

33.1

/

1000

are positively correlated since 

ğ¶ğ¶

ğ‘™ğ‘™ğ‘šğ‘šğ‘ğ‘ğ‘¡ğ‘¡ ğµğµ, ğ¶ğ¶ < 1

.

35

ğ‘™ğ‘™ğ‘šğ‘šğ‘ğ‘ğ‘¡ğ‘¡ ğµğµ, Â¬ğ¶ğ¶ > 1

ğµğµ
Â¬ğ¶ğ¶

ğµğµ

Interestingness Measure: 

ï¯ To test correlated events: 

â€¢
â€¢

2

: independent
: correlated, either positive or 

=

ğœ’ğœ’

2

ğœ’ğœ’
negative ïƒ  needs additional test
ğœ’ğœ’

= 0
> 0

ğŸğŸ
ğŒğŒ
âˆ‘ ğ‘‚ğ‘‚ğ‘‚ğ‘‚ğ‘ ğ‘ ğ‘‚ğ‘‚ğ‘‚ğ‘‚ğ‘‚ğ‘‚ğ‘‚ğ‘‚ğ‘‚ğ‘‚ âˆ’ğ¸ğ¸ğ¸ğ¸ğ¸ğ¸ğ‘‚ğ‘‚ğ¸ğ¸ğ¸ğ¸ğ‘‚ğ‘‚ğ‘‚ğ‘‚
B
ğ¸ğ¸ğ¸ğ¸ğ¸ğ¸ğ‘‚ğ‘‚ğ¸ğ¸ğ¸ğ¸ğ‘‚ğ‘‚ğ‘‚ğ‘‚

2

2

Not B

C

400 (450) 350 (300)

Not C

200 (150)

50 (100)

sum

600

400

sum

750

250

1000

ï¯ Thus, 

and 

are negatively correlated since the expected

Expected value

Observed value

value is 450 but the observed is only 400.

ğµğµ

ğ¶ğ¶

is also more telling than the support-confidence framework

36

ï¯

2

ğœ’ğœ’

Lift and 

: Are They Always Good Measures?

ï¯ Null transactions: Transactions that contain neither 

2

ğœ’ğœ’

Examine the dataset:
â€¢

is much rarer than 

nor 

ğ‘©ğ‘©

ğ‘ªğ‘ª

and 
ğµğµğ¶ğ¶ 100, 0.1%
& 

â€¢ There are many 
â€¢ Unlikely 

Â¬ğµğµğ¶ğ¶ 1000

.
will happen together!
Â¬ğµğµÂ¬ğ¶ğ¶ 100000, 98%

ğµğµÂ¬ğ¶ğ¶ 1000

ğµğµ

ğ¶ğ¶

ï± However, B and C seem to be strongly positively 

correlated based on:

ï±
ï±

and Observed (100) >> 

2

ğ‘™ğ‘™ğ‘šğ‘šğ‘ğ‘ğ‘¡ğ‘¡ ğµğµ, ğ¶ğ¶ = 8.44 â‰« 1
Expected (11.85)
ğœ’ğœ’

(ğµğµ, ğ¶ğ¶) = 670

null transactions

Contingency table with 
expected values added

ï± Too many null transactions may â€œspoil the soupâ€!

37

Interestingness Measures: Null-Invariant

ï¯ Null invariance: value does not change with # null-transactions

ï®

and 

are NOT null-invariant with the range of 

.

ï¯ Null-invariant Measures:
ğ‘™ğ‘™ğ‘šğ‘šğ‘ğ‘ğ‘¡ğ‘¡

ğœ’ğœ’

2

0, âˆ

ï® All Confidence: the minimum confidence of the two association rules 

related to A and B, namely, â€œA ïƒ  Bâ€ and â€œB ïƒ  Aâ€

ï® Max Confidence: the maximum confidence of the two rules
ï® Kulczynski (Kulc): an average of two confidence values
ï® Cosine: a harmonized lift measure (unaffected by # total transactions)

38

Null Invariance: An Example

ï¯ Why is null invariance crucial for the analysis of transactions? 

ï® Many transactions may not contain any itemsets being examined.

milk vs. coffee contingency table

Null-transactions 
w.r.t. milk and coffee

are not null-invariant 

Lift and 
â€“ not good to evaluate data that 
ğŒğŒ
contain either too many or too few 
null transactions!

ğŸğŸ

39

Comparison of Null-Invariant Measures

ï¯ Not all null-invariant measures are created equal.
ï¯ Which one is better?

milk vs. coffee contingency table

â€¢ Kulc (Kulczynski 1927) holds firm and is in 
balance of both directional implications.

Subtle: They disagree 
on those cases

40

Summary

ï¯ The discovery of frequent patterns, associations, and correlation 

relationships is useful in many applications.
ï® Customersâ€™ buying habits: itemsets that are frequently bought together
, min_sup);  

ï¯ Association rule mining: 1) frequent k-itemsets (

2) generating strong association (
ğ´ğ´ âˆª ğµğµ
ï® Apriori: any subset of a frequent itemset must be frequent!
ï® Efficiency bottleneck: reduce # candidates or DB scans

, min_conf). 

ğ´ğ´ â†’ ğµğµ

ï¯ Not all strong association rules are interesting. 

ï® The supportâ€“confidence framework vs other interestingness measures 
ï® A measure is null-invariant if its value is free from the influence of null-

transactions (that do not contain any itemsets being examined).

41

Email: fengmei.jin@polyu.edu.hk

Office: PQ747

THANK YOU!

42

