COMP5121
Data Mining and Data Warehousing Applications

Week 10: Outlier Detection

Dr. Fengmei Jin
 Email: fengmei.jin@polyu.edu.hk
 Office: PQ747 (+852 3400 3327)
 Consultation Hours: 2.30-4.30pm every Thursday

Outline

 Outliers and Outlier Analysis
 Outlier Detection Methods Categories
 Statistical Methods
 Proximity-Based Methods
 Clustering-Based Methods
 Mining Contextual and Collective Outliers
 Outlier Detection in High-Dimensional Space

2

OUTLIERS AND OUTLIER ANALYSIS

3

What are Outliers?

 Outlier: A data object that deviates significantly from the rest of 
the objects, as if it were generated by a different mechanism
 Unusual transaction target/amount
 Temperature 
 …

Sales

We often refer the rest of the 
object as normal data and 
outliers as abnormal data.

Test Score

4

What are Outliers?

 Outliers are different from the noises

 Noises are random errors or variance in a measurement process. 
 Noises can mislead data analysis and need to be removed.

 Outliers are interesting

 Provide new knowledge
 Potentially be influential
 Need to be handled with care

5

Types of Outliers – Global 

 Global outlier: A data object that deviates significantly from the 

entire dataset

6

Types of Outlier – Contextual

 Contextual outlier: A data object deviating significantly with 

respect to a specific context of the object

 Example: Is 25℃ in March an outlier?

 In Hong Kong, it is normal.
 In Moscow, …

7

Types of Outliers – Collective 

 Collective outlier: A subset of data objects that collectively 

deviates significantly from the entire dataset

 Key point: A single data point may not be an outlier on its own, 

but their combined behavior makes them unusual.
 Example: A sudden spike in network traffic from a group of 

devices might indicate a cyber attack.

8

Types of Outliers

 A dataset can have multiple types of outliers

 Different outliers may be used in different applications

 Global: simplest but may not be accurate
 Contextual: require domain knowledge
 Collective: model the behavior of a group of data objects

9

Challenges of Outlier Detection (I)

 Modeling normal objects and outliers properly

 The quality of detection depends on how well we model normal 

data and outliers.

 It is almost impossible to enumerate all normal data in a dataset.
 The boundary between “normal” and “abnormal” is not clear.

 Application-specific outlier detection

 The choice of distance measure and relationship between 

objects are often application-dependent.

 It is impossible to develop a universal outlier detection method.

10

Challenges of Outlier Detection (II)

 Handling noise in outlier detection

 Outlier provides valuable insights while noise doesn’t.
 Noise may distort the normal objects and blur the distinction 
between normal objects and outliers, making detection hard.

 Understandability

 Understand why these are outliers: justification of the detection
 Specify the degree of an outlier: how unlikely it is for the object to 

be generated by a normal mechanism

11

CATEGORIZATION: OUTLIER 
DETECTION METHODS

12

Categorization: Different Criteria

 Based on the data labels

 Supervised, Unsupervised, Semi-supervised (partial labels)

 Based on assumption regarding normal data vs outliers

 Statistical: normal data are generated from a statistical model
 Proximity-based: outliers are far away from their nearest 

neighbors compared to normal data

 Clustering-based: normal data belong to large, dense clusters; 

outliers belong to small, sparse clusters, or no clusters

13

(I) Supervised Methods

 Modeling outlier detection as a classification problem

 Samples examined by domain experts used for training & testing
 To learn a classifier for outlier detection effectively:

 Model normal objects and report those not matching the model as outliers, or
 Model outliers and treat those not matching the model as normal

 Challenges

 Imbalanced classes: Outliers are rare  Boost the outlier class 

by generating some artificial outliers for training

 Recall > Precision: Catch as many outliers as possible, even if it 

means misclassifying some normal objects as outliers

14

(I) Unsupervised Methods

 Intuition: assume the normal objects are somewhat clustered 

into multiple groups, each having some distinct features
 Outliers are expected to be far away from any normal groups

 Weakness

 Normal objects may not share any strong patterns, but the 
collective outliers may share high similarity in a small area
 Unsupervised methods may have a high false positive rate but 

still miss many real outliers.

 Hard to distinguish noise from outliers
 Clustering is expensive, but far fewer outliers than normal objects 

15

(I) Semi-Supervised Methods

 Situation: in many applications, # labeled data is often limited
 Labels could be on outliers only, normal objects only, or both.

 If labeled normal objects are available:

 Use the labeled examples and the nearby unlabeled objects to 

train a model for normal objects

 Those not fitting the normal model are flagged as outliers

 If labeled outliers are available:

 A small number of labeled outliers may not represent all outliers
 Combine with unsupervised methods to learn a model of normal 

objects and improve detection accuracy.

16

(II) Statistical Methods (model-based)

 Assume normal data follow some statistical/stochastic models.

 Data that do not conform to the model are outliers.

 Effectiveness: highly depends on whether the assumption of 

statistical model holds in the real data

 Statistical modeling

 Parametric: Assume a specific distribution (e.g., Gaussian).
 Non-parametric: Do not assume a specific distribution, offering 

more flexibility.

17

(II) Proximity-Based Methods

 An object is an outlier if its nearest neighbors of the object are 

farther away compared to most other objects
 Proximity: measured by comparing its distance to its neighbors. 
 If the object’s proximity significantly deviates from the proximity of 
most other objects in the same set, it is flagged as an outlier.

 Effectiveness: highly relies on the proximity measure

 Defining proximity measures can be difficult in some applications.
 Struggles with groups of outliers that are close to each other.
 Two types: distance-based vs. density-based (density of objects 

in the surrounding area)

18

(II) Clustering-Based Methods

 Normal data belong to large, dense clusters
 Outliers belong to small or sparse clusters, or no clusters

 Challenges

 Clustering is expensive: Clustering methods often have high 

computational costs, especially for large datasets.

 Scalability: Straightforward clustering may not scale well to large 

or high-dimensional datasets.

19

parametric vs non-parametric
STATISTICAL METHODS

20

Statistical Methods

 Assume that the normal objects in a data set are generated by 

a stochastic process or a generative model

 Categories

 Parametric method assumes that the normal data objects are 

generated by a parametric distribution with parameter 
 Example: Gaussian distribution, Poisson distribution.

 Non-parametric does not assume an a priori statistical model
 Example: Kernel Density Estimation (KDE), histogram-based

𝜃𝜃

21

Parametric Method – Normal Distribution

 Widely used in statistics and natural/social sciences to model 

real-valued random variables with unknown distribution
 Represented by the probability density function (PDF):

 Notation: 

2

−

1
2

𝑥𝑥−𝜇𝜇
𝜎𝜎

𝑓𝑓 𝑥𝑥 =
2

1

𝜎𝜎 2𝜋𝜋

𝑒𝑒

𝑋𝑋~𝑁𝑁 𝜇𝜇, 𝜎𝜎
Normal distributions are common in:
• Adult heights
IQ scores
•
• Measurement errors
• …

22

Parametric Method – Normal Distribution

68-95-99.7 Rule: describes the percentage of data falling within 1, 
2, or 3 standard deviations of the mean:

 Given a random variable 





2

𝑋𝑋~𝑁𝑁(𝜇𝜇, 𝜎𝜎

)

𝑃𝑃(𝜇𝜇 − 𝜎𝜎 ≤ 𝑋𝑋 ≤ 𝜇𝜇 + 𝜎𝜎) ≈ 68.27%
𝑃𝑃(𝜇𝜇 − 2𝜎𝜎 ≤ 𝑋𝑋 ≤ 𝜇𝜇 + 2𝜎𝜎) ≈ 95.45%
𝑃𝑃(𝜇𝜇 − 3𝜎𝜎 ≤ 𝑋𝑋 ≤ 𝜇𝜇 + 3𝜎𝜎) ≈ 99.73%

•
•

is the mean (center)

is the variance (spread)

2

𝜇𝜇
𝜎𝜎

23

Parametric Method – Normal Distribution

 Given a dataset 

:

 Estimate mean: 

 Estimate variance: 

𝑥𝑥1, 𝑥𝑥2, … , 𝑥𝑥𝑛𝑛
1
𝑛𝑛 ∑ 𝑥𝑥𝑖𝑖
1
𝑛𝑛 ∑ 𝑥𝑥𝑖𝑖 − ̅𝑥𝑥

̂𝜇𝜇 = ̅𝑥𝑥 =
2

=

�𝜎𝜎

2

 Outlier detection using 65-95-99.7 rule

 A data object 
 Only 0.3% of data lies beyond 3 standard deviations from mean. 
 This makes 

𝑥𝑥𝑖𝑖
highly unlikely belong to this normal distribution.

is considered an outlier if 

𝑥𝑥𝑖𝑖 − ̅𝑥𝑥 > 3𝜎𝜎

𝑥𝑥𝑖𝑖

24

Parametric Method – IQR and Boxplot

 Given a dataset 
 Calculate 
 Calculate the interquartile range 
𝑄𝑄2
 Outliers: Any data point outside 

(lower quartile), 
𝑥𝑥1, 𝑥𝑥2, … , 𝑥𝑥𝑛𝑛

𝑄𝑄1

(median), 

(upper quartile)

𝑄𝑄3

𝐼𝐼𝑄𝑄𝐼𝐼 = 𝑄𝑄3 − 𝑄𝑄1
[𝑄𝑄1 − 1.5 × 𝐼𝐼𝑄𝑄𝐼𝐼, 𝑄𝑄3 + 1.5 × 𝐼𝐼𝑄𝑄𝐼𝐼]

 Key idea: Similar to 68-95-99.7 rule, the 

range captures most normal data
 The parameter 

is a typical threshold

but could be adjusted accordingly
1.5

25

Parametric Method –

Statistic

 Multivariate data: data involving two or more attributes
𝝌𝝌

 Transform it to univariate statistic for easier outlier detection

𝟐𝟐

 Given a data object 

 Calculate 

2

𝑜𝑜 = (𝑜𝑜1, 𝑜𝑜2, … , 𝑜𝑜𝑑𝑑)
𝑜𝑜𝑖𝑖−𝐸𝐸𝑖𝑖
𝐸𝐸𝑖𝑖



𝜒𝜒
 The larger 
𝑜𝑜

is the observed value and 
= ∑
is, the more likely 

is the expected value
is an outlier

𝐸𝐸

2

2

𝜒𝜒

𝑜𝑜

26

Non-parametric Method – Histogram 

 Construct a histogram from the dataset using bins
 Example: A transaction over $5,000 can be an outlier since 

only 

of transactions is over $5,000

0.2%

 Challenge: hard to choose bin size

 Too small 
 Too big 

Normal data in rare bins

Outliers in frequent bins
⇒

⇒

27

Non-parametric Method – KDE

 Kernel Density Estimation: A method used to estimate the 

probability density distribution of the data
 Every data object contributes to the probability density of others.
 The contribution of a data object to another object decreases as their 

distance increases.

 KDE-based outlier detection:

, calculate the estimated probability density 

 Bandwidth: 

, the larger the bandwidth, the smoother the estimated pdf

 Given a dataset 
function, that is, 

 The lower 

ℎ

̂𝑓𝑓ℎ 𝑥𝑥

𝑥𝑥1, 𝑥𝑥2, … , 𝑥𝑥𝑛𝑛
𝑥𝑥−𝑥𝑥𝑖𝑖
̂𝑓𝑓ℎ 𝑥𝑥 =
ℎ )
is, the more unlikely

1
𝑛𝑛ℎ ∑ 𝐾𝐾(

is generated from the distribution
28

𝑥𝑥

Non-parametric Method – KDE

 Compared to histogram:

 Smoothness: KDE provides a continuous density estimation, 

while histograms are discrete.

 Flexibility: KDE does not rely on fixed bin sizes, reducing the 

sensitivity to bin width.

 Edge effect: KDE minimizes abrupt changes at the boundaries.

29

Summary: Statistical Methods

 Pros

 Statistically justifiable: providing interpretable and reliable results
 Once the distribution is learned, detection process is fast

 Cons

 Learning process is slow especially for complex distributions
 Not suitable for high-dimensional data

30

PROXIMITY-BASED METHODS

31

Proximity-based Outlier Detection

 Proximity: the degree of nearness/closeness between objects

 In data science, it refers to similarity or dissimilarity (distance)
 Euclidean distance, cosine similarity, jaccard similarity, etc.

 Assumption: the proximity of an outlier object to its nearest 

neighbors significantly deviates from the proximity of the object 
to most other objects in the dataset

32

Nearest Neighbors (NN)

 The nearest neighbor to a data object 

is 

the data object closest to 

𝑜𝑜

𝑜𝑜

 We can extend this concept to 

-nearest 

neighbors.

𝑘𝑘

Voronoi Diagram

33

Distance-Based Outlier

 Given a data object 

and a distance threshold 

, its 

-

neighborhood is defined as 

𝑜𝑜

𝑁𝑁𝑟𝑟 = {𝑜𝑜

′

′

|𝑜𝑜

≠ 𝑜𝑜 ∧ 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑 𝑜𝑜

𝑟𝑟 ≥ 0
′

𝑟𝑟
, 𝑜𝑜 ≤ 𝑟𝑟}

 A data object 

is a 

-outlier if 

𝑜𝑜

 Fraction threshold: 
 It suggests 

𝐷𝐷𝐷𝐷(𝑟𝑟, 𝜋𝜋)
is an outlier if its 
data points compared to the total dataset.

0 < 𝜋𝜋 ≤ 1

𝑜𝑜

𝑟𝑟

|𝑁𝑁𝑟𝑟|
|𝐷𝐷| < 𝜋𝜋

-neighborhood contains too few 

34

Distance-Based Method: A Nested Loop Algorithm

 For each data object 

, let 

for 

1. Calculate 

2. If 

𝑜𝑜𝑖𝑖
, then 
𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑(𝑜𝑜𝑗𝑗, 𝑜𝑜𝑖𝑖)
, exit
3. If 
𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑 𝑜𝑜𝑗𝑗, 𝑜𝑜𝑖𝑖 ≤ 𝑟𝑟
4. Repeat from Step 1
𝑐𝑐𝑜𝑜𝑐𝑐𝑐𝑐𝑑𝑑 ≥ 𝜋𝜋|𝐷𝐷|
 If not exit before, then 

𝑐𝑐𝑜𝑜𝑐𝑐𝑐𝑐𝑑𝑑 ← 0

𝑗𝑗 ≠ 𝑑𝑑

𝑐𝑐𝑜𝑜𝑐𝑐𝑐𝑐𝑑𝑑 ← 𝑐𝑐𝑜𝑜𝑐𝑐𝑐𝑐𝑑𝑑 + 1

is a 

-outlier 

𝑜𝑜𝑖𝑖

𝐷𝐷𝐷𝐷(𝑟𝑟, 𝜋𝜋)

35

Distance-Based Method: A k-NN Algorithm

 Determine 

-outlier with 

-nearest neighbors

 A data object 

neighbor exceeds the distance threshold 
 The number of neighbors is determined by: 

is an outlier if the distance to its 
𝑘𝑘
, i.e., 

𝐷𝐷𝐷𝐷(𝑟𝑟, 𝜋𝜋)
𝑜𝑜

-th nearest 

𝑘𝑘
𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑 𝑜𝑜𝑘𝑘, 𝑜𝑜 > 𝑟𝑟

𝑟𝑟

𝑘𝑘 = 𝜋𝜋|𝐷𝐷|

 Advantages: Simple and interpretable. Works well for datasets 

where proximity is meaningful.

 Challenges: Computationally expensive for large datasets and 

may struggle with high-dimensional data.

36

Density-Based Method

 Distance-based methods discover global outliers

 The 
 Controlled by two global parameters 

-outlier is far from 

𝐷𝐷𝐷𝐷(𝑟𝑟, 𝜋𝜋)

and 

1 − 𝜋𝜋 × 100%

of data objects

 Density-based methods assume the density around an outlier 

𝑟𝑟

𝜋𝜋

object is significantly different from the density around its 
neighbors. 

37

to be the distance 
-th nearest 

Density-Based Method

 We define 
between 
neighbor

 Note that 
𝑁𝑁𝑘𝑘 𝑜𝑜 = {𝑜𝑜
than 

𝑜𝑜

′

and its 
𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑘𝑘 𝑜𝑜

𝑘𝑘

|𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑 𝑜𝑜

data objects.
𝑁𝑁𝑘𝑘 𝑜𝑜

′

can contain more 
, 𝑜𝑜 ≤ 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑘𝑘(𝑜𝑜)}

 Local density: the average distance 

𝑘𝑘

in 

to 
 It is sensitive to small distance

𝑜𝑜

𝑁𝑁𝑘𝑘 𝑜𝑜

is significantly 
If the local density of 
lower than its nearest neighbors, it is 
an outlier.

𝑜𝑜

38

Summary: Proximity-Based Methods

 Pros

 Understandable to humans
 Non-parametric. No assumptions on the data distribution
 Flexible to different proximity measurements

 Cons

 Computation cost can be high especially in high-dimensional 

space

 Not suitable for collective outliers detection

41

CLUSTERING-BASED METHODS

42

Clustering-Based Outlier Detection

 Clustering-based methods examine the relationship between 

data objects and clusters

1.
2.
3.

If a data object doesn’t belong to any cluster, it is an outlier
If a data object is far from its nearest cluster, it is an outlier
If a data object belongs to a small or sparse cluster, all objects in 
that cluster are outliers

43

Case 1: Not Belong to Any Cluster

 Using clustering methods like DBSCAN, some data points may 

not belong to any cluster.

 These unclustered points are considered outliers.

 Consider organizing a library. Most books fit into well-defined 

categories, such as "Fiction" or "Science.“ 

 However, a rare, unrelated book that doesn’t belong to any 
category (like a handwritten manuscript) would be an outlier.

44

Case 2: Far Away from Nearest Cluster



-means clustering is sensitive to outliers, as points far from 

cluster centers may distort the clustering process.
𝑘𝑘

 We can find the data objects who is far away from the cluster 

center as outliers
 Unusual activity (e.g., rare login attempts or abnormal patterns) 
often appears far from established clusters of normal behavior.

45

Case 3: Outliers in Small Clusters

 The above two cases focus on detecting individual outliers
 Cluster-based local outlier factor (CBLOF)
 Clusters are divided into large and small 

 based on # objects they cover

 For an object 

in large cluster: 

is the cluster 

lies in

𝑜𝑜
 For an object 

𝐶𝐶

in small cluster: 
𝑜𝑜
is the nearest large cluster of 





𝐶𝐶𝐷𝐷𝐶𝐶𝐶𝐶𝐶𝐶 = # 𝑜𝑜𝑜𝑜𝑗𝑗𝑒𝑒𝑐𝑐𝑑𝑑𝑑𝑑 × 𝑑𝑑𝑑𝑑𝑠𝑠(𝑜𝑜, 𝐶𝐶)

𝑜𝑜

𝐶𝐶𝐷𝐷𝐶𝐶𝐶𝐶𝐶𝐶 = # 𝑜𝑜𝑜𝑜𝑗𝑗𝑒𝑒𝑐𝑐𝑑𝑑𝑑𝑑 × 𝑑𝑑𝑑𝑑𝑠𝑠(𝑜𝑜, 𝐶𝐶)

𝐶𝐶

𝑜𝑜

46

Summary: Clustering-Based Methods 

 Pros

 Unsupervised. Suitable for any type of data
 Clusters can be regarded as a summary of data and help other 

tasks.

 Detection process is fast as # cluster is typically small

 Cons

 Effectiveness is limited since the labels are missing.

47

MINING CONTEXTUAL AND COLLECTIVE 
OUTLIERS

48

Contextual Outlier Detection

 The attributes of data objects are divided into two groups

 Context attribute: e.g. longitude, latitude, time, etc.
 Behavioral attribute: e.g. temperature

 How to analyze the corresponding contextual information?
 In some scenarios, the contexts can not be clearly identified

49

Extending Conventional Outlier Detection

 When the contexts can be clearly identified

 Identify the contexts of data objects using contextual attributes
 Apply a conventional outlier detection

Example: Is 28℃ an outlier for Hong 
Kong in April?
• First find all data objects whose “City” equals 

“Hong Kong” and “Month” equals “April”
• Apply a conventional outlier detection on 

these selected data objects

City

Hong Kong

Hong Kong

Tokyo

……

Month

April

April

March

……

Temperature
28.7℃

26℃

12℃

……

50

Modeling Normal Behavior wrt Contexts

 When the contexts cannot be clearly identified

 E.g. finding an abnormal purchase wrt to the browser log
 There is no straightforward way to determine how much of a 

customer's browsing history should be considered

 Use a predictive model to predict the purchase based on the 

browser log
 If an actual purchase is significantly different from the prediction, 

it can be considered as an outlier。

51

Modeling Behavior – Regression Analysis

 Regression analysis reveals the correlation between data 

objects
 Assume the purchase highly depends on the browser log
 Learn a regression model to predict purchase behavior based on 

browser log

 Find outliers if actual purchases deviate significantly from prediction

52

Modeling Behavior – Markov Models

 Markov property: 
 The number 

 Based on Markov property, we can learn a Markov model to 

is called the order
𝑃𝑃 𝑋𝑋𝑛𝑛 𝑋𝑋𝑛𝑛−1, … , 𝑋𝑋1 = 𝑃𝑃(𝑋𝑋𝑛𝑛|𝑋𝑋𝑛𝑛−1, … , 𝑋𝑋𝑛𝑛−𝑘𝑘)

represent the transition probability from one product to another, 
or from one product to the purchase

𝑘𝑘

53

Modeling Behavior – LSTM

 To handle the fixed-order issue of Markov models, we can use 

a recurrent neural network (RNN) called LSTM

 Long Short-Term Memory (LSTM) network aims to provide a 

short-term memory for RNN that can last thousands of 
timesteps
 LSTM is good at time-series data prediction

54

Collective Outlier Detection

 A group of data objects as a whole deviating significant from 

the entire dataset

 Question: How can we identify the abnormal behavior of a 

group of data objects
 We need to examine the structure of the dataset

55

Collective Outlier Detection

 Structure

 Temporal data: sub-sequences
 Spatial data: local areas
 Graph and network data: subgraphs

56

Extending Conventional Outlier Detection

 We first break the entire dataset into small “structure units”

 Sub sequences, local areas, subgraphs, etc.

 Then we can conduct conventional outlier detection on these 

“structured objects”

Example: Graph outlier detection
• Structure units: subgraphs with different 

number of nodes

• Typically, the number of subgraphs decreases 

while more nodes and edges are involved
• Find the outlier with unexpected #subgraph

57

Modeling the Normal Behavior

 In real scenarios, the number of structure units can be huge 

and it’s impossible to examine all of them

 We can use predictive models (Markov models, LSTM, etc.) to 

model and predict normal data

58

OUTLIER IN HIGH-DIMENSIONAL SPACE

59

Curse of Dimensionality & High Dimensions

 Data objects are sparse in high-dimensional space

 Hard to understand the structure of data: making it difficult to 

cluster, classify, or understand relationships.

 Sensitive to noises: Small variations in high-dimensional data can 

drastically affect results due to the sparsity.

 Visualization challenge: Humans can’t intuitively visualize 

data beyond three dimensions.

60

Curse of Dimensionality & High Dimensions

 Distance metrics: The distance becomes meaningless!

 In high dimensions, the difference between the nearest and 

farthest neighbors diminishes.



lim𝑑𝑑→∞ 𝐸𝐸

𝑑𝑑𝑖𝑖𝑑𝑑𝑡𝑡𝑚𝑚𝑚𝑚𝑚𝑚−𝑑𝑑𝑖𝑖𝑑𝑑𝑡𝑡𝑚𝑚𝑖𝑖𝑚𝑚
𝑑𝑑𝑖𝑖𝑑𝑑𝑡𝑡𝑚𝑚𝑖𝑖𝑚𝑚

→ 0

 Combinatorial explosion: High-dimensional data leads to an 
exponential increase in # feature combinations to analyze.
 30-dimensional space 
 Most attributes are irrelevant attributes

one billion possible combination!

30

2

≈

61

Challenges for High-Dimensional Outlier Detection

 Detecting outliers without saying why they are outliers is not 

very useful in high-dimensional space.
 Many dimensions may have irrelevant or noisy features. 

 Data sparsity: Noise dominates, making it hard to distinguish 

true outliers from noisy data.

 Subspaces and Scalability

 Outliers often exist in specific subspaces, not in the full space. 
 Efficient exploration of subspaces is vital for meaningful outliers.

62

Extending Conventional Outlier Detection

 Method 1: Detect outliers in the full space (e.g., HilOut*)

1. Find distance-based outliers, but use the ranks of distance 

instead of the absolute distance in outlier detection

2. For each object 

, find its 

-nearest neighbors (

denotes 

“nearest neighbors”): 

𝑜𝑜

𝑘𝑘

3. Compute the weight of object 

𝑘𝑘
4. Rank all objects in weight-descending order
𝑤𝑤 𝑜𝑜 = ∑𝑖𝑖=1
5. Select the top-

objects as outliers (

𝑐𝑐𝑐𝑐1 𝑜𝑜 , … , 𝑐𝑐𝑐𝑐𝑘𝑘 𝑜𝑜
𝑜𝑜

: 

𝑐𝑐𝑐𝑐

𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑 𝑜𝑜, 𝑐𝑐𝑐𝑐𝑖𝑖 𝑜𝑜

: user-specified parameter)

𝑙𝑙

𝑙𝑙

*Angiulli, Fabrizio, and Clara Pizzuti. "Outlier mining in large high-dimensional data sets." IEEE TKDE. 17.2 (2005): 203-215.

63

Extending Conventional Outlier Detection

 Method 2: Dimensionality 

reduction
 PCA-based Heuristic Approach: 
Principal components with low 
variance are preferred because:
 Normal objects tend to cluster 
closely in these selected 
dimensions

 Outliers are more likely to deviate 
significantly from the majority.

64

Finding Outliers in Subspaces

 Detecting outliers in full-dimensional spaces is hard to interpret.

 Find outliers in much lower dimensional subspaces

 Understand why the object is an outlier
 Quantify to what extent it deviates from normal behavior
 Example: Find “outlier customers” in certain subspace

 average transaction amount >> avg.
 purchase frequency << avg.

65

Finding Outliers in Subspaces: Example

 A grid-based subspace outlier detection method to identify an 

area with significantly lower density than the average
 Project data onto various subspaces
 Discretize the data into a grid with equal-depth regions

 For each dimension, create 

partitions  each contains 

of total 

data

𝜙𝜙

 Search for regions that are significantly sparse

1

𝜙𝜙

𝑓𝑓 =

 For an 

-dimensional subspace, the expected # objects in a cell is 

 Calculate a sparsity score 

𝑟𝑟

that may contain outliers
𝑟𝑟

𝑆𝑆 𝐶𝐶 =

𝑓𝑓

1−𝑓𝑓

𝑛𝑛

𝑛𝑛 𝐶𝐶 −𝑓𝑓

𝑟𝑟

𝑛𝑛
𝑟𝑟

sparse cell 

𝐶𝐶

, where 

𝑟𝑟

means a 
𝑐𝑐

𝑓𝑓

𝑆𝑆 𝐶𝐶 < 0

66

Modeling High-Dimensional Outliers

 Develop new models for high-dimensional outliers directly

 Avoid proximity measures and adopt new heuristics that do not 

deteriorate in high-dimensional data

A set of points 
form a cluster 
except c (outlier)

67

Summary

 Types of outliers: Global, Contextual, Collective
 Detection methods: Supervised/Unsupervised/Semi-supervised

 Statistical methods: Parametric / Nonparametric
 Proximity-based methods: Distance- / Density-based
 Clustering-based methods: Three cases
 High-dimensional outlier detection

68

Email: fengmei.jin@polyu.edu.hk

Office: PQ747

THANK YOU!

69

