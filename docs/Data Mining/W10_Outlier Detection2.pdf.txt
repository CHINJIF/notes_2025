COMP5121
Data Mining and Data Warehousing Applications

Week 10: Outlier Detection

Dr. Fengmei Jin
ï‚§ Email: fengmei.jin@polyu.edu.hk
ï‚§ Office: PQ747 (+852 3400 3327)
ï‚§ Consultation Hours: 2.30-4.30pm every Thursday

Outline

ï¯ Outliers and Outlier Analysis
ï¯ Outlier Detection Methods Categories
ï¯ Statistical Methods
ï¯ Proximity-Based Methods
ï¯ Clustering-Based Methods
ï¯ Mining Contextual and Collective Outliers
ï¯ Outlier Detection in High-Dimensional Space

2

OUTLIERS AND OUTLIER ANALYSIS

3

What are Outliers?

ï¯ Outlier: A data object that deviates significantly from the rest of 
the objects, as if it were generated by a different mechanism
ï® Unusual transaction target/amount
ï® Temperature 
ï® â€¦

Sales

We often refer the rest of the 
object as normal data and 
outliers as abnormal data.

Test Score

4

What are Outliers?

ï¯ Outliers are different from the noises

ï® Noises are random errors or variance in a measurement process. 
ï® Noises can mislead data analysis and need to be removed.

ï¯ Outliers are interesting

ï® Provide new knowledge
ï® Potentially be influential
ï® Need to be handled with care

5

Types of Outliers â€“ Global 

ï¯ Global outlier: A data object that deviates significantly from the 

entire dataset

6

Types of Outlier â€“ Contextual

ï¯ Contextual outlier: A data object deviating significantly with 

respect to a specific context of the object

ï¯ Example: Is 25â„ƒ in March an outlier?

ï® In Hong Kong, it is normal.
ï® In Moscow, â€¦

7

Types of Outliers â€“ Collective 

ï¯ Collective outlier: A subset of data objects that collectively 

deviates significantly from the entire dataset

ï¯ Key point: A single data point may not be an outlier on its own, 

but their combined behavior makes them unusual.
ï® Example: A sudden spike in network traffic from a group of 

devices might indicate a cyber attack.

8

Types of Outliers

ï¯ A dataset can have multiple types of outliers

ï¯ Different outliers may be used in different applications

ï® Global: simplest but may not be accurate
ï® Contextual: require domain knowledge
ï® Collective: model the behavior of a group of data objects

9

Challenges of Outlier Detection (I)

ï¯ Modeling normal objects and outliers properly

ï® The quality of detection depends on how well we model normal 

data and outliers.

ï® It is almost impossible to enumerate all normal data in a dataset.
ï® The boundary between â€œnormalâ€ and â€œabnormalâ€ is not clear.

ï¯ Application-specific outlier detection

ï® The choice of distance measure and relationship between 

objects are often application-dependent.

ï® It is impossible to develop a universal outlier detection method.

10

Challenges of Outlier Detection (II)

ï¯ Handling noise in outlier detection

ï® Outlier provides valuable insights while noise doesnâ€™t.
ï® Noise may distort the normal objects and blur the distinction 
between normal objects and outliers, making detection hard.

ï¯ Understandability

ï® Understand why these are outliers: justification of the detection
ï® Specify the degree of an outlier: how unlikely it is for the object to 

be generated by a normal mechanism

11

CATEGORIZATION: OUTLIER 
DETECTION METHODS

12

Categorization: Different Criteria

ï¯ Based on the data labels

ï® Supervised, Unsupervised, Semi-supervised (partial labels)

ï¯ Based on assumption regarding normal data vs outliers

ï® Statistical: normal data are generated from a statistical model
ï® Proximity-based: outliers are far away from their nearest 

neighbors compared to normal data

ï® Clustering-based: normal data belong to large, dense clusters; 

outliers belong to small, sparse clusters, or no clusters

13

(I) Supervised Methods

ï¯ Modeling outlier detection as a classification problem

ï® Samples examined by domain experts used for training & testing
ï® To learn a classifier for outlier detection effectively:

ï¯ Model normal objects and report those not matching the model as outliers, or
ï¯ Model outliers and treat those not matching the model as normal

ï¯ Challenges

ï® Imbalanced classes: Outliers are rare ïƒ  Boost the outlier class 

by generating some artificial outliers for training

ï® Recall > Precision: Catch as many outliers as possible, even if it 

means misclassifying some normal objects as outliers

14

(I) Unsupervised Methods

ï¯ Intuition: assume the normal objects are somewhat clustered 

into multiple groups, each having some distinct features
ï® Outliers are expected to be far away from any normal groups

ï¯ Weakness

ï® Normal objects may not share any strong patterns, but the 
collective outliers may share high similarity in a small area
ï® Unsupervised methods may have a high false positive rate but 

still miss many real outliers.

ï® Hard to distinguish noise from outliers
ï® Clustering is expensive, but far fewer outliers than normal objects 

15

(I) Semi-Supervised Methods

ï¯ Situation: in many applications, # labeled data is often limited
ï® Labels could be on outliers only, normal objects only, or both.

ï¯ If labeled normal objects are available:

ï® Use the labeled examples and the nearby unlabeled objects to 

train a model for normal objects

ï® Those not fitting the normal model are flagged as outliers

ï¯ If labeled outliers are available:

ï® A small number of labeled outliers may not represent all outliers
ï® Combine with unsupervised methods to learn a model of normal 

objects and improve detection accuracy.

16

(II) Statistical Methods (model-based)

ï¯ Assume normal data follow some statistical/stochastic models.

ï® Data that do not conform to the model are outliers.

ï¯ Effectiveness: highly depends on whether the assumption of 

statistical model holds in the real data

ï¯ Statistical modeling

ï® Parametric: Assume a specific distribution (e.g., Gaussian).
ï® Non-parametric: Do not assume a specific distribution, offering 

more flexibility.

17

(II) Proximity-Based Methods

ï¯ An object is an outlier if its nearest neighbors of the object are 

farther away compared to most other objects
ï® Proximity: measured by comparing its distance to its neighbors. 
ï® If the objectâ€™s proximity significantly deviates from the proximity of 
most other objects in the same set, it is flagged as an outlier.

ï¯ Effectiveness: highly relies on the proximity measure

ï® Defining proximity measures can be difficult in some applications.
ï® Struggles with groups of outliers that are close to each other.
ï® Two types: distance-based vs. density-based (density of objects 

in the surrounding area)

18

(II) Clustering-Based Methods

ï¯ Normal data belong to large, dense clusters
ï¯ Outliers belong to small or sparse clusters, or no clusters

ï¯ Challenges

ï® Clustering is expensive: Clustering methods often have high 

computational costs, especially for large datasets.

ï® Scalability: Straightforward clustering may not scale well to large 

or high-dimensional datasets.

19

parametric vs non-parametric
STATISTICAL METHODS

20

Statistical Methods

ï¯ Assume that the normal objects in a data set are generated by 

a stochastic process or a generative model

ï¯ Categories

ï® Parametric method assumes that the normal data objects are 

generated by a parametric distribution with parameter 
ï¯ Example: Gaussian distribution, Poisson distribution.

ï® Non-parametric does not assume an a priori statistical model
ï¯ Example: Kernel Density Estimation (KDE), histogram-based

ğœƒğœƒ

21

Parametric Method â€“ Normal Distribution

ï¯ Widely used in statistics and natural/social sciences to model 

real-valued random variables with unknown distribution
ï® Represented by the probability density function (PDF):

ï® Notation: 

2

âˆ’

1
2

ğ‘¥ğ‘¥âˆ’ğœ‡ğœ‡
ğœğœ

ğ‘“ğ‘“ ğ‘¥ğ‘¥ =
2

1

ğœğœ 2ğœ‹ğœ‹

ğ‘’ğ‘’

ğ‘‹ğ‘‹~ğ‘ğ‘ ğœ‡ğœ‡, ğœğœ
Normal distributions are common in:
â€¢ Adult heights
IQ scores
â€¢
â€¢ Measurement errors
â€¢ â€¦

22

Parametric Method â€“ Normal Distribution

68-95-99.7 Rule: describes the percentage of data falling within 1, 
2, or 3 standard deviations of the mean:

ï¯ Given a random variable 

ï®
ï®
ï®

2

ğ‘‹ğ‘‹~ğ‘ğ‘(ğœ‡ğœ‡, ğœğœ

)

ğ‘ƒğ‘ƒ(ğœ‡ğœ‡ âˆ’ ğœğœ â‰¤ ğ‘‹ğ‘‹ â‰¤ ğœ‡ğœ‡ + ğœğœ) â‰ˆ 68.27%
ğ‘ƒğ‘ƒ(ğœ‡ğœ‡ âˆ’ 2ğœğœ â‰¤ ğ‘‹ğ‘‹ â‰¤ ğœ‡ğœ‡ + 2ğœğœ) â‰ˆ 95.45%
ğ‘ƒğ‘ƒ(ğœ‡ğœ‡ âˆ’ 3ğœğœ â‰¤ ğ‘‹ğ‘‹ â‰¤ ğœ‡ğœ‡ + 3ğœğœ) â‰ˆ 99.73%

â€¢
â€¢

is the mean (center)

is the variance (spread)

2

ğœ‡ğœ‡
ğœğœ

23

Parametric Method â€“ Normal Distribution

ï¯ Given a dataset 

:

ï® Estimate mean: 

ï® Estimate variance: 

ğ‘¥ğ‘¥1, ğ‘¥ğ‘¥2, â€¦ , ğ‘¥ğ‘¥ğ‘›ğ‘›
1
ğ‘›ğ‘› âˆ‘ ğ‘¥ğ‘¥ğ‘–ğ‘–
1
ğ‘›ğ‘› âˆ‘ ğ‘¥ğ‘¥ğ‘–ğ‘– âˆ’ Ì…ğ‘¥ğ‘¥

Ì‚ğœ‡ğœ‡ = Ì…ğ‘¥ğ‘¥ =
2

=

ï¿½ğœğœ

2

ï¯ Outlier detection using 65-95-99.7 rule

ï® A data object 
ï® Only 0.3% of data lies beyond 3 standard deviations from mean. 
ï® This makes 

ğ‘¥ğ‘¥ğ‘–ğ‘–
highly unlikely belong to this normal distribution.

is considered an outlier if 

ğ‘¥ğ‘¥ğ‘–ğ‘– âˆ’ Ì…ğ‘¥ğ‘¥ > 3ğœğœ

ğ‘¥ğ‘¥ğ‘–ğ‘–

24

Parametric Method â€“ IQR and Boxplot

ï¯ Given a dataset 
ï® Calculate 
ï® Calculate the interquartile range 
ğ‘„ğ‘„2
ï® Outliers: Any data point outside 

(lower quartile), 
ğ‘¥ğ‘¥1, ğ‘¥ğ‘¥2, â€¦ , ğ‘¥ğ‘¥ğ‘›ğ‘›

ğ‘„ğ‘„1

(median), 

(upper quartile)

ğ‘„ğ‘„3

ğ¼ğ¼ğ‘„ğ‘„ğ¼ğ¼ = ğ‘„ğ‘„3 âˆ’ ğ‘„ğ‘„1
[ğ‘„ğ‘„1 âˆ’ 1.5 Ã— ğ¼ğ¼ğ‘„ğ‘„ğ¼ğ¼, ğ‘„ğ‘„3 + 1.5 Ã— ğ¼ğ¼ğ‘„ğ‘„ğ¼ğ¼]

ï¯ Key idea: Similar to 68-95-99.7 rule, the 

range captures most normal data
ï® The parameter 

is a typical threshold

but could be adjusted accordingly
1.5

25

Parametric Method â€“

Statistic

ï¯ Multivariate data: data involving two or more attributes
ğŒğŒ

ï® Transform it to univariate statistic for easier outlier detection

ğŸğŸ

ï¯ Given a data object 

ï® Calculate 

2

ğ‘œğ‘œ = (ğ‘œğ‘œ1, ğ‘œğ‘œ2, â€¦ , ğ‘œğ‘œğ‘‘ğ‘‘)
ğ‘œğ‘œğ‘–ğ‘–âˆ’ğ¸ğ¸ğ‘–ğ‘–
ğ¸ğ¸ğ‘–ğ‘–

ï¯

ğœ’ğœ’
ï® The larger 
ğ‘œğ‘œ

is the observed value and 
= âˆ‘
is, the more likely 

is the expected value
is an outlier

ğ¸ğ¸

2

2

ğœ’ğœ’

ğ‘œğ‘œ

26

Non-parametric Method â€“ Histogram 

ï¯ Construct a histogram from the dataset using bins
ï¯ Example: A transaction over $5,000 can be an outlier since 

only 

of transactions is over $5,000

0.2%

ï¯ Challenge: hard to choose bin size

ï® Too small 
ï® Too big 

Normal data in rare bins

Outliers in frequent bins
â‡’

â‡’

27

Non-parametric Method â€“ KDE

ï¯ Kernel Density Estimation: A method used to estimate the 

probability density distribution of the data
ï® Every data object contributes to the probability density of others.
ï® The contribution of a data object to another object decreases as their 

distance increases.

ï¯ KDE-based outlier detection:

, calculate the estimated probability density 

ï¯ Bandwidth: 

, the larger the bandwidth, the smoother the estimated pdf

ï® Given a dataset 
function, that is, 

ï® The lower 

â„

Ì‚ğ‘“ğ‘“â„ ğ‘¥ğ‘¥

ğ‘¥ğ‘¥1, ğ‘¥ğ‘¥2, â€¦ , ğ‘¥ğ‘¥ğ‘›ğ‘›
ğ‘¥ğ‘¥âˆ’ğ‘¥ğ‘¥ğ‘–ğ‘–
Ì‚ğ‘“ğ‘“â„ ğ‘¥ğ‘¥ =
â„ )
is, the more unlikely

1
ğ‘›ğ‘›â„ âˆ‘ ğ¾ğ¾(

is generated from the distribution
28

ğ‘¥ğ‘¥

Non-parametric Method â€“ KDE

ï¯ Compared to histogram:

ï® Smoothness: KDE provides a continuous density estimation, 

while histograms are discrete.

ï® Flexibility: KDE does not rely on fixed bin sizes, reducing the 

sensitivity to bin width.

ï® Edge effect: KDE minimizes abrupt changes at the boundaries.

29

Summary: Statistical Methods

ï¯ Pros

ï® Statistically justifiable: providing interpretable and reliable results
ï® Once the distribution is learned, detection process is fast

ï¯ Cons

ï® Learning process is slow especially for complex distributions
ï® Not suitable for high-dimensional data

30

PROXIMITY-BASED METHODS

31

Proximity-based Outlier Detection

ï¯ Proximity: the degree of nearness/closeness between objects

ï¯ In data science, it refers to similarity or dissimilarity (distance)
ï® Euclidean distance, cosine similarity, jaccard similarity, etc.

ï¯ Assumption: the proximity of an outlier object to its nearest 

neighbors significantly deviates from the proximity of the object 
to most other objects in the dataset

32

Nearest Neighbors (NN)

ï¯ The nearest neighbor to a data object 

is 

the data object closest to 

ğ‘œğ‘œ

ğ‘œğ‘œ

ï¯ We can extend this concept to 

-nearest 

neighbors.

ğ‘˜ğ‘˜

Voronoi Diagram

33

Distance-Based Outlier

ï¯ Given a data object 

and a distance threshold 

, its 

-

neighborhood is defined as 

ğ‘œğ‘œ

ğ‘ğ‘ğ‘Ÿğ‘Ÿ = {ğ‘œğ‘œ

â€²

â€²

|ğ‘œğ‘œ

â‰  ğ‘œğ‘œ âˆ§ ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ ğ‘œğ‘œ

ğ‘Ÿğ‘Ÿ â‰¥ 0
â€²

ğ‘Ÿğ‘Ÿ
, ğ‘œğ‘œ â‰¤ ğ‘Ÿğ‘Ÿ}

ï¯ A data object 

is a 

-outlier if 

ğ‘œğ‘œ

ï® Fraction threshold: 
ï® It suggests 

ğ·ğ·ğ·ğ·(ğ‘Ÿğ‘Ÿ, ğœ‹ğœ‹)
is an outlier if its 
data points compared to the total dataset.

0 < ğœ‹ğœ‹ â‰¤ 1

ğ‘œğ‘œ

ğ‘Ÿğ‘Ÿ

|ğ‘ğ‘ğ‘Ÿğ‘Ÿ|
|ğ·ğ·| < ğœ‹ğœ‹

-neighborhood contains too few 

34

Distance-Based Method: A Nested Loop Algorithm

ï¯ For each data object 

, let 

for 

1. Calculate 

2. If 

ğ‘œğ‘œğ‘–ğ‘–
, then 
ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘(ğ‘œğ‘œğ‘—ğ‘—, ğ‘œğ‘œğ‘–ğ‘–)
, exit
3. If 
ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ ğ‘œğ‘œğ‘—ğ‘—, ğ‘œğ‘œğ‘–ğ‘– â‰¤ ğ‘Ÿğ‘Ÿ
4. Repeat from Step 1
ğ‘ğ‘ğ‘œğ‘œğ‘ğ‘ğ‘ğ‘ğ‘‘ğ‘‘ â‰¥ ğœ‹ğœ‹|ğ·ğ·|
ï¯ If not exit before, then 

ğ‘ğ‘ğ‘œğ‘œğ‘ğ‘ğ‘ğ‘ğ‘‘ğ‘‘ â† 0

ğ‘—ğ‘— â‰  ğ‘‘ğ‘‘

ğ‘ğ‘ğ‘œğ‘œğ‘ğ‘ğ‘ğ‘ğ‘‘ğ‘‘ â† ğ‘ğ‘ğ‘œğ‘œğ‘ğ‘ğ‘ğ‘ğ‘‘ğ‘‘ + 1

is a 

-outlier 

ğ‘œğ‘œğ‘–ğ‘–

ğ·ğ·ğ·ğ·(ğ‘Ÿğ‘Ÿ, ğœ‹ğœ‹)

35

Distance-Based Method: A k-NN Algorithm

ï¯ Determine 

-outlier with 

-nearest neighbors

ï® A data object 

neighbor exceeds the distance threshold 
ï® The number of neighbors is determined by: 

is an outlier if the distance to its 
ğ‘˜ğ‘˜
, i.e., 

ğ·ğ·ğ·ğ·(ğ‘Ÿğ‘Ÿ, ğœ‹ğœ‹)
ğ‘œğ‘œ

-th nearest 

ğ‘˜ğ‘˜
ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ ğ‘œğ‘œğ‘˜ğ‘˜, ğ‘œğ‘œ > ğ‘Ÿğ‘Ÿ

ğ‘Ÿğ‘Ÿ

ğ‘˜ğ‘˜ = ğœ‹ğœ‹|ğ·ğ·|

ï¯ Advantages: Simple and interpretable. Works well for datasets 

where proximity is meaningful.

ï¯ Challenges: Computationally expensive for large datasets and 

may struggle with high-dimensional data.

36

Density-Based Method

ï¯ Distance-based methods discover global outliers

ï® The 
ï® Controlled by two global parameters 

-outlier is far from 

ğ·ğ·ğ·ğ·(ğ‘Ÿğ‘Ÿ, ğœ‹ğœ‹)

and 

1 âˆ’ ğœ‹ğœ‹ Ã— 100%

of data objects

ï¯ Density-based methods assume the density around an outlier 

ğ‘Ÿğ‘Ÿ

ğœ‹ğœ‹

object is significantly different from the density around its 
neighbors. 

37

to be the distance 
-th nearest 

Density-Based Method

ï¯ We define 
between 
neighbor
ï®
ï® Note that 
ğ‘ğ‘ğ‘˜ğ‘˜ ğ‘œğ‘œ = {ğ‘œğ‘œ
than 

ğ‘œğ‘œ

â€²

and its 
ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘˜ğ‘˜ ğ‘œğ‘œ

ğ‘˜ğ‘˜

|ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ ğ‘œğ‘œ

data objects.
ğ‘ğ‘ğ‘˜ğ‘˜ ğ‘œğ‘œ

â€²

can contain more 
, ğ‘œğ‘œ â‰¤ ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘˜ğ‘˜(ğ‘œğ‘œ)}

ï¯ Local density: the average distance 

ğ‘˜ğ‘˜

in 

to 
ï® It is sensitive to small distance

ğ‘œğ‘œ

ğ‘ğ‘ğ‘˜ğ‘˜ ğ‘œğ‘œ

is significantly 
If the local density of 
lower than its nearest neighbors, it is 
an outlier.

ğ‘œğ‘œ

38

Summary: Proximity-Based Methods

ï¯ Pros

ï® Understandable to humans
ï® Non-parametric. No assumptions on the data distribution
ï® Flexible to different proximity measurements

ï¯ Cons

ï® Computation cost can be high especially in high-dimensional 

space

ï® Not suitable for collective outliers detection

41

CLUSTERING-BASED METHODS

42

Clustering-Based Outlier Detection

ï¯ Clustering-based methods examine the relationship between 

data objects and clusters

1.
2.
3.

If a data object doesnâ€™t belong to any cluster, it is an outlier
If a data object is far from its nearest cluster, it is an outlier
If a data object belongs to a small or sparse cluster, all objects in 
that cluster are outliers

43

Case 1: Not Belong to Any Cluster

ï¯ Using clustering methods like DBSCAN, some data points may 

not belong to any cluster.

ï¯ These unclustered points are considered outliers.

ï® Consider organizing a library. Most books fit into well-defined 

categories, such as "Fiction" or "Science.â€œ 

ï® However, a rare, unrelated book that doesnâ€™t belong to any 
category (like a handwritten manuscript) would be an outlier.

44

Case 2: Far Away from Nearest Cluster

ï¯

-means clustering is sensitive to outliers, as points far from 

cluster centers may distort the clustering process.
ğ‘˜ğ‘˜

ï¯ We can find the data objects who is far away from the cluster 

center as outliers
ï® Unusual activity (e.g., rare login attempts or abnormal patterns) 
often appears far from established clusters of normal behavior.

45

Case 3: Outliers in Small Clusters

ï¯ The above two cases focus on detecting individual outliers
ï¯ Cluster-based local outlier factor (CBLOF)
ï® Clusters are divided into large and small 

ï¯ based on # objects they cover

ï® For an object 

in large cluster: 

is the cluster 

lies in

ğ‘œğ‘œ
ï® For an object 

ğ¶ğ¶

in small cluster: 
ğ‘œğ‘œ
is the nearest large cluster of 

ï¯

ï¯

ğ¶ğ¶ğ·ğ·ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ = # ğ‘œğ‘œğ‘œğ‘œğ‘—ğ‘—ğ‘’ğ‘’ğ‘ğ‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ Ã— ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘ ğ‘ (ğ‘œğ‘œ, ğ¶ğ¶)

ğ‘œğ‘œ

ğ¶ğ¶ğ·ğ·ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ = # ğ‘œğ‘œğ‘œğ‘œğ‘—ğ‘—ğ‘’ğ‘’ğ‘ğ‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ Ã— ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘ ğ‘ (ğ‘œğ‘œ, ğ¶ğ¶)

ğ¶ğ¶

ğ‘œğ‘œ

46

Summary: Clustering-Based Methods 

ï¯ Pros

ï® Unsupervised. Suitable for any type of data
ï® Clusters can be regarded as a summary of data and help other 

tasks.

ï® Detection process is fast as # cluster is typically small

ï¯ Cons

ï® Effectiveness is limited since the labels are missing.

47

MINING CONTEXTUAL AND COLLECTIVE 
OUTLIERS

48

Contextual Outlier Detection

ï¯ The attributes of data objects are divided into two groups

ï® Context attribute: e.g. longitude, latitude, time, etc.
ï® Behavioral attribute: e.g. temperature

ï¯ How to analyze the corresponding contextual information?
ï® In some scenarios, the contexts can not be clearly identified

49

Extending Conventional Outlier Detection

ï¯ When the contexts can be clearly identified

ï® Identify the contexts of data objects using contextual attributes
ï® Apply a conventional outlier detection

Example: Is 28â„ƒ an outlier for Hong 
Kong in April?
â€¢ First find all data objects whose â€œCityâ€ equals 

â€œHong Kongâ€ and â€œMonthâ€ equals â€œAprilâ€
â€¢ Apply a conventional outlier detection on 

these selected data objects

City

Hong Kong

Hong Kong

Tokyo

â€¦â€¦

Month

April

April

March

â€¦â€¦

Temperature
28.7â„ƒ

26â„ƒ

12â„ƒ

â€¦â€¦

50

Modeling Normal Behavior wrt Contexts

ï¯ When the contexts cannot be clearly identified

ï® E.g. finding an abnormal purchase wrt to the browser log
ï® There is no straightforward way to determine how much of a 

customer's browsing history should be considered

ï¯ Use a predictive model to predict the purchase based on the 

browser log
ï® If an actual purchase is significantly different from the prediction, 

it can be considered as an outlierã€‚

51

Modeling Behavior â€“ Regression Analysis

ï¯ Regression analysis reveals the correlation between data 

objects
ï® Assume the purchase highly depends on the browser log
ï® Learn a regression model to predict purchase behavior based on 

browser log

ï® Find outliers if actual purchases deviate significantly from prediction

52

Modeling Behavior â€“ Markov Models

ï¯ Markov property: 
ï® The number 

ï¯ Based on Markov property, we can learn a Markov model to 

is called the order
ğ‘ƒğ‘ƒ ğ‘‹ğ‘‹ğ‘›ğ‘› ğ‘‹ğ‘‹ğ‘›ğ‘›âˆ’1, â€¦ , ğ‘‹ğ‘‹1 = ğ‘ƒğ‘ƒ(ğ‘‹ğ‘‹ğ‘›ğ‘›|ğ‘‹ğ‘‹ğ‘›ğ‘›âˆ’1, â€¦ , ğ‘‹ğ‘‹ğ‘›ğ‘›âˆ’ğ‘˜ğ‘˜)

represent the transition probability from one product to another, 
or from one product to the purchase

ğ‘˜ğ‘˜

53

Modeling Behavior â€“ LSTM

ï¯ To handle the fixed-order issue of Markov models, we can use 

a recurrent neural network (RNN) called LSTM

ï¯ Long Short-Term Memory (LSTM) network aims to provide a 

short-term memory for RNN that can last thousands of 
timesteps
ï® LSTM is good at time-series data prediction

54

Collective Outlier Detection

ï¯ A group of data objects as a whole deviating significant from 

the entire dataset

ï¯ Question: How can we identify the abnormal behavior of a 

group of data objects
ï® We need to examine the structure of the dataset

55

Collective Outlier Detection

ï¯ Structure

ï® Temporal data: sub-sequences
ï® Spatial data: local areas
ï® Graph and network data: subgraphs

56

Extending Conventional Outlier Detection

ï¯ We first break the entire dataset into small â€œstructure unitsâ€

ï® Sub sequences, local areas, subgraphs, etc.

ï¯ Then we can conduct conventional outlier detection on these 

â€œstructured objectsâ€

Example: Graph outlier detection
â€¢ Structure units: subgraphs with different 

number of nodes

â€¢ Typically, the number of subgraphs decreases 

while more nodes and edges are involved
â€¢ Find the outlier with unexpected #subgraph

57

Modeling the Normal Behavior

ï¯ In real scenarios, the number of structure units can be huge 

and itâ€™s impossible to examine all of them

ï¯ We can use predictive models (Markov models, LSTM, etc.) to 

model and predict normal data

58

OUTLIER IN HIGH-DIMENSIONAL SPACE

59

Curse of Dimensionality & High Dimensions

ï¯ Data objects are sparse in high-dimensional space

ï® Hard to understand the structure of data: making it difficult to 

cluster, classify, or understand relationships.

ï® Sensitive to noises: Small variations in high-dimensional data can 

drastically affect results due to the sparsity.

ï¯ Visualization challenge: Humans canâ€™t intuitively visualize 

data beyond three dimensions.

60

Curse of Dimensionality & High Dimensions

ï¯ Distance metrics: The distance becomes meaningless!

ï® In high dimensions, the difference between the nearest and 

farthest neighbors diminishes.

ï®

limğ‘‘ğ‘‘â†’âˆ ğ¸ğ¸

ğ‘‘ğ‘‘ğ‘–ğ‘–ğ‘‘ğ‘‘ğ‘¡ğ‘¡ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘šâˆ’ğ‘‘ğ‘‘ğ‘–ğ‘–ğ‘‘ğ‘‘ğ‘¡ğ‘¡ğ‘šğ‘šğ‘–ğ‘–ğ‘šğ‘š
ğ‘‘ğ‘‘ğ‘–ğ‘–ğ‘‘ğ‘‘ğ‘¡ğ‘¡ğ‘šğ‘šğ‘–ğ‘–ğ‘šğ‘š

â†’ 0

ï¯ Combinatorial explosion: High-dimensional data leads to an 
exponential increase in # feature combinations to analyze.
ï® 30-dimensional space ïƒ 
ï® Most attributes are irrelevant attributes

one billion possible combination!

30

2

â‰ˆ

61

Challenges for High-Dimensional Outlier Detection

ï¯ Detecting outliers without saying why they are outliers is not 

very useful in high-dimensional space.
ï® Many dimensions may have irrelevant or noisy features. 

ï¯ Data sparsity: Noise dominates, making it hard to distinguish 

true outliers from noisy data.

ï¯ Subspaces and Scalability

ï® Outliers often exist in specific subspaces, not in the full space. 
ï® Efficient exploration of subspaces is vital for meaningful outliers.

62

Extending Conventional Outlier Detection

ï¯ Method 1: Detect outliers in the full space (e.g., HilOut*)

1. Find distance-based outliers, but use the ranks of distance 

instead of the absolute distance in outlier detection

2. For each object 

, find its 

-nearest neighbors (

denotes 

â€œnearest neighborsâ€): 

ğ‘œğ‘œ

ğ‘˜ğ‘˜

3. Compute the weight of object 

ğ‘˜ğ‘˜
4. Rank all objects in weight-descending order
ğ‘¤ğ‘¤ ğ‘œğ‘œ = âˆ‘ğ‘–ğ‘–=1
5. Select the top-

objects as outliers (

ğ‘ğ‘ğ‘ğ‘1 ğ‘œğ‘œ , â€¦ , ğ‘ğ‘ğ‘ğ‘ğ‘˜ğ‘˜ ğ‘œğ‘œ
ğ‘œğ‘œ

: 

ğ‘ğ‘ğ‘ğ‘

ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ ğ‘œğ‘œ, ğ‘ğ‘ğ‘ğ‘ğ‘–ğ‘– ğ‘œğ‘œ

: user-specified parameter)

ğ‘™ğ‘™

ğ‘™ğ‘™

*Angiulli, Fabrizio, and Clara Pizzuti. "Outlier mining in large high-dimensional data sets." IEEE TKDE. 17.2 (2005): 203-215.

63

Extending Conventional Outlier Detection

ï¯ Method 2: Dimensionality 

reduction
ï® PCA-based Heuristic Approach: 
Principal components with low 
variance are preferred because:
ï¯ Normal objects tend to cluster 
closely in these selected 
dimensions

ï¯ Outliers are more likely to deviate 
significantly from the majority.

64

Finding Outliers in Subspaces

ï¯ Detecting outliers in full-dimensional spaces is hard to interpret.

ï¯ Find outliers in much lower dimensional subspaces

ï® Understand why the object is an outlier
ï® Quantify to what extent it deviates from normal behavior
ï® Example: Find â€œoutlier customersâ€ in certain subspace

ï¯ average transaction amount >> avg.
ï¯ purchase frequency << avg.

65

Finding Outliers in Subspaces: Example

ï¯ A grid-based subspace outlier detection method to identify an 

area with significantly lower density than the average
ï® Project data onto various subspaces
ï® Discretize the data into a grid with equal-depth regions

ï¯ For each dimension, create 

partitions ïƒ  each contains 

of total 

data

ğœ™ğœ™

ï® Search for regions that are significantly sparse

1

ğœ™ğœ™

ğ‘“ğ‘“ =

ï¯ For an 

-dimensional subspace, the expected # objects in a cell is 

ï¯ Calculate a sparsity score 

ğ‘Ÿğ‘Ÿ

that may contain outliers
ğ‘Ÿğ‘Ÿ

ğ‘†ğ‘† ğ¶ğ¶ =

ğ‘“ğ‘“

1âˆ’ğ‘“ğ‘“

ğ‘›ğ‘›

ğ‘›ğ‘› ğ¶ğ¶ âˆ’ğ‘“ğ‘“

ğ‘Ÿğ‘Ÿ

ğ‘›ğ‘›
ğ‘Ÿğ‘Ÿ

sparse cell 

ğ¶ğ¶

, where 

ğ‘Ÿğ‘Ÿ

means a 
ğ‘ğ‘

ğ‘“ğ‘“

ğ‘†ğ‘† ğ¶ğ¶ < 0

66

Modeling High-Dimensional Outliers

ï¯ Develop new models for high-dimensional outliers directly

ï® Avoid proximity measures and adopt new heuristics that do not 

deteriorate in high-dimensional data

A set of points 
form a cluster 
except c (outlier)

67

Summary

ï¯ Types of outliers: Global, Contextual, Collective
ï¯ Detection methods: Supervised/Unsupervised/Semi-supervised

ï® Statistical methods: Parametric / Nonparametric
ï® Proximity-based methods: Distance- / Density-based
ï® Clustering-based methods: Three cases
ï® High-dimensional outlier detection

68

Email: fengmei.jin@polyu.edu.hk

Office: PQ747

THANK YOU!

69

